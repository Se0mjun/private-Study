# 1. 확률 이론과 강화학습

불확실성은 사건의 각각이 일어날 Likelihood와 probability로 나타낼 수 있다.
발생할 수 있는 모든 상황을 world라고 생각할 수 있으며, w로 나타낸다. 예를 들면 주사위를 던진 결과는 6 world가 될 수 있으며, 
특정 세계의 확률을 P(w)로 나타낸다.

### 1.1 확률의 공리
확률을 나타내는 값은 0부터 1까지의 범위에 들어야한다.  $$0 <= P(w) <=1$$  

0은 불가능한 사건을 의미하며, 1은 무조건 일어나는 사건  

$$\Sigma  P(w) = 1$$

### 1.2 조건부 확률
이미 밝혀진 일부 증거가 주어졌을 때, 명제에 대한 믿음의 정도이다.
사건이 미래에 발생할 확률에 영향을 미치는 정보를 사용하기 위해 조건부 확률에 의존한다.  

$$P(A|B) =  \frac {P(A \cap B)}{P(B)}$$

### 1.3 베이즈 정리 (Bayes Rule)

사건 B가 발생한다는 가정하에 사건 A가 발생할 확률을 사건 A의 조건부 확률이라고 하고 아래와 같이 정리할 수 있다.  

$$P(A|B) =  \frac {P(A , B)}{P(B)}$$

이와 비슷하게 사건 A가 발생한다는 가정하에 사건 B가 발생할 확률은 아래와 같다.  

$$P(B|A) = \frac {P(A , B)}{P(A)}$$

위의 두 식을 이용하여 chain rule을 만들면
$$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$$

우측 그림과 같이 N개의 사건이 서로 배타적이고 $\Sigma_{i=1} ^n P(B_i) = 1$ 이면, 
임의의 사건 A의 확률은 아래와 같다.  

$$P(A) = \Sigma_{i=1} ^n P(A, B_i) = \Sigma_{i=1} ^n P(A|B_i)P(B_i)$$

위의 식을 total probability라고 하며, 사건 A를 조건으로 하는 임의의 사건 $B_i$의 조건부 확률을 Chain Rule을 이용해 표현하면  

$$P(B_i|A) = \frac {P(A,B_i)} {P(A)} = \frac {P(A|B_i)P(B_i)} {P(A)}$$  

위의 식을 total probability 정리를 대입하면  

$$P(B_i|A) = \frac {P(A|B_i)P(B_i)} {\Sigma_{i=1} ^nP(A|B_i)P(B_i)}$$ 

### 1.4 강화학습에서의 확률적 모델링

#### 1.4.1 상태 전이 확률
MDP(Markov Decision Process)에서 상태 전이 확률 P(s'|s,a)는 현재 상태 s에서 행동 a를 취했을 때 다음 상태 s'로 전이될 확률을 나타낸다:
$$P(s'|s,a) = P(S_{t+1}=s'|S_t=s, A_t=a)$$

이 전이 확률은 환경의 동적 특성을 모델링하며, 마르코프 성질을 따른다.

#### 1.4.2 정책과 확률
정책 π는 각 상태에서 어떤 행동을 취할지 결정하는 확률 분포이다:

- 확정적 정책: π(s) = a
- 확률적 정책: π(a|s) = P(A_t=a|S_t=s)

확률적 정책은 탐험과 이용 사이의 균형을 조절하고, 더 안정적인 학습을 가능하게 한다.

#### 1.4.3 기대값과 확률 분포
강화학습에서 가치 함수(value function)는 기대값의 형태로 표현된다:

$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s\right]$$

$$Q^\pi(s,a) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s, A_t = a\right]$$

이 기대값은 정책 π에 따라 행동할 때의 확률 분포를 따른다.

### 1.5 불확실성 처리

#### 1.5.1 탐험과 이용의 확률적 균형
ε-greedy 전략은 확률적으로 탐험과 이용 사이의 균형을 맞춘다:

- ε의 확률로 무작위 행동 선택 (탐험)
- 1-ε의 확률로 최적의 행동 선택 (이용)

#### 1.5.2 엔트로피와 탐험
엔트로피는 확률 분포의 불확실성을 측정하는 지표이다:
$$H(\pi(\cdot|s)) = -\sum_a \pi(a|s) \log \pi(a|s)$$

최신 알고리즘(SAC 등)에서는 엔트로피를 목적 함수에 추가하여 탐험을 장려한다:
$$J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_t r(s_t,a_t) + \alpha H(\pi_\theta(\cdot|s_t))\right]$$

#### 1.5.3 베이지안 강화학습
베이지안 접근법은 모델 파라미터의 불확실성을 명시적으로 모델링한다:

- 전이 확률, 보상 함수에 대한 사전 확률(prior) 설정
- 경험을 통해 posterior 분포 업데이트
- Thompson 샘플링 등을 통한 효율적 탐험

### 1.6 고급 확률적 개념

#### 1.6.1 중요도 샘플링(Importance Sampling)
다른 분포에서 샘플링된 데이터를 사용하여 목표 분포의 기대값을 추정하는 기법이다:
$$\mathbb{E}_{p(x)}[f(x)] = \mathbb{E}_{q(x)}\left[f(x)\frac{p(x)}{q(x)}\right]$$

Off-policy 학습에서 중요도 샘플링은 이전 정책의 경험으로부터 현재 정책을 평가하는 데 사용된다.

#### 1.6.2 KL 발산과 정책 제약
KL 발산은 두 확률 분포 간의 차이를 측정한다:
$$D_{KL}(p||q) = \sum_x p(x) \log\frac{p(x)}{q(x)}$$

TRPO, PPO 등의 알고리즘에서는 정책 업데이트 시 KL 발산 제약을 사용하여 급격한 정책 변화를 방지한다:
$$\max_\theta \mathbb{E}_{\pi_{\theta_{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A^{\pi_{\theta_{old}}}(s,a)\right] \text{ s.t. } D_{KL}(\pi_{\theta_{old}}||\pi_\theta) \leq \delta$$

#### 1.6.3 확률적 벨만 방정식
확률적 환경에서의 벨만 방정식은 다음과 같다:
$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V^\pi(s')]$$

$$Q^\pi(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$$

이러한 방정식들은 강화학습의 이론적 기반을 형성하며, 다양한 알고리즘의 수렴성을 보장한다.

### 1.7 실제 응용에서의 확률

#### 1.7.1 확률적 가치 추정
몬테카를로 방법과 Temporal Difference 학습은 경험 샘플링을 통해 가치 함수를 확률적으로 추정한다:

- 몬테카를로: 전체 에피소드의 실제 리턴 사용
- TD: 부트스트래핑을 통한 추정값 업데이트

#### 1.7.2 확률적 행동 선택
실제 강화학습 시스템에서는 다양한 확률적 행동 선택 메커니즘을 사용한다:

- Softmax 정책: $$\pi(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}$$
- Gaussian 정책 (연속 행동 공간): $$\pi(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta^2(s))$$

#### 1.7.3 분포적 강화학습
분포적 강화학습(Distributional RL)은 가치의 단일 기대값 대신 전체 확률 분포를 학습한다:

- 리턴 분포의 불확실성 캡처
- 위험 인식(risk-aware) 의사결정 가능
- 더 정보가 풍부한 학습 신호 제공

이 접근법은 C51, QR-DQN, IQN 등의 알고리즘에서 구현되었으며, 학습 성능과 안정성을 개선한다.

# 2. Markov 모델과 강화학습의 수학적 기반

Markov 모델은 확률론적 과정들을 설명하는 수학적 모델로, 강화학습의 이론적 토대를 형성합니다. 이 모델은 현재 상태가 주어졌을 때 미래 상태가 과거 상태와 독립적이라는 Markov 속성에 기반합니다.

## 2.1 Markov 속성의 정의

Markov 속성은 다음 상태가 오직 현재 상태에만 의존하고 이전 상태들의 이력에는 의존하지 않는 확률 과정의 특성입니다:

$$P(s_{t+1} | s_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t)$$

이는 "미래는 현재만 알면 예측할 수 있으며, 과거는 더 이상 영향을 미치지 않는다"는 의미입니다. 이 속성은 복잡한 시스템을 모델링할 때 문제를 단순화하는 강력한 가정입니다.

## 2.2 Markov 과정과 Markov 체인

### 2.2.1 Markov 과정
Markov 과정은 Markov 속성을 만족하는 상태 전이의 확률적 모델입니다. 이산 시간 Markov 과정에서 시스템은 각 시간 단계마다 상태를 변경할 수 있으며, 다음 상태로의 전이 확률은 현재 상태에만 의존합니다.

### 2.2.2 Markov 체인
Markov 체인은 유한한 상태 공간을 가진 Markov 과정입니다. 각 상태에서 다른 상태로의 전이 확률은 상태 전이 행렬 P로 표현됩니다:

$$P_{ij} = P(s_{t+1} = j | s_t = i)$$

여기서 $P_{ij}$는 상태 i에서 상태 j로 전이할 확률입니다.

## 2.3 Markov 모델의 유형

Markov 모델은 제어 가능성과 상태 관찰 가능성에 따라 네 가지 주요 유형으로 분류됩니다:

### 2.3.1 Markov 체인 (Markov Chain)

- **제어 가능성**: ✖ (제어 없음)
- **상태 관찰 가능성**: ✔ (완전 관찰)
- **특징**: 
  - 에이전트가 상태 전이에 영향을 미칠 수 없음
  - 모든 상태가 완전히 관찰 가능
- **수학적 표현**: 상태 전이 행렬 $P$로 정의
- **응용 예시**: 날씨 예측, 금융 시장 모델링, 인구 변동 분석

### 2.3.2 은닉 Markov 모델 (Hidden Markov Model, HMM)

- **제어 가능성**: ✖ (제어 없음)
- **상태 관찰 가능성**: ✖ (부분 관찰)
- **특징**: 
  - 실제 상태는 직접 관찰할 수 없고 관측 값을 통해 간접적으로 추론
  - 관측 행렬 $O$가 추가됨
- **수학적 표현**: $(S, O, A, B, \pi)$
  - $S$: 은닉 상태 집합
  - $O$: 관측 가능한 출력 집합
  - $A$: 상태 전이 확률 행렬
  - $B$: 관측 확률 행렬
  - $\pi$: 초기 상태 분포
- **응용 예시**: 음성 인식, 자연어 처리, 생물정보학

### 2.3.3 Markov 결정 과정 (Markov Decision Process, MDP)

- **제어 가능성**: ✔ (에이전트가 행동함)
- **상태 관찰 가능성**: ✔ (완전 관찰)
- **특징**: 
  - 에이전트가 행동을 선택하여 상태 전이에 영향을 미침
  - 보상 메커니즘 포함
- **수학적 표현**: $(S, A, P, R, \gamma)$
  - $S$: 상태 공간
  - $A$: 행동 공간
  - $P$: 상태 전이 확률 함수
  - $R$: 보상 함수
  - $\gamma$: 할인 인자
- **응용 예시**: 게임 AI, 로봇 제어, 자원 할당

### 2.3.4 부분 관측 가능 Markov 결정 과정 (Partially Observable MDP, POMDP)

- **제어 가능성**: ✔ (에이전트가 행동함)
- **상태 관찰 가능성**: ✖ (부분 관찰)
- **특징**: 
  - 에이전트가 행동을 선택할 수 있지만 환경의 상태를 완전히 관찰할 수 없음
  - 믿음 상태(belief state)를 통해 상태 추론
- **수학적 표현**: $(S, A, O, P, Z, R, \gamma)$
  - MDP 요소 + $O$(관측 공간) + $Z$(관측 확률)
- **응용 예시**: 센서 기반 로봇 제어, 불확실성이 있는 환경에서의 의사결정, 대화 시스템

## 2.4 Markov 모델의 비교표

| 모델 | 제어 가능성 | 상태 관찰 가능성 | 주요 응용 | 특징 |
|------|-------------|------------------|---------|------|
| **Markov Chain** | ✖ | ✔ | 날씨 예측 | 가장 단순한 형태, 자율적 시스템 모델링 |
| **HMM** | ✖ | ✖ | 음성 인식 | 상태 추정 및 시퀀스 예측에 유용 |
| **MDP** | ✔ | ✔ | 게임, 로봇 제어 | 강화학습의 기본 프레임워크 |
| **POMDP** | ✔ | ✖ | 센서 기반 로봇 제어 | 가장 일반적이고 복잡한 형태, 불확실성 처리 |

## 2.5 Markov 모델과 강화학습의 관계

강화학습 알고리즘은 대부분 MDP 프레임워크를 기반으로 합니다. 핵심 관계는 다음과 같습니다:

1. **상태 표현**: Markov 속성은 상태가 미래 행동에 필요한 모든 정보를 포함해야 함을 의미
2. **최적 정책 탐색**: 강화학습은 MDP에서 최적 정책을 찾는 과정
3. **가치 함수**: 벨만 방정식은 Markov 속성을 기반으로 상태 및 행동 가치 함수를 재귀적으로 정의
4. **알고리즘 선택**: 환경의 관찰 가능성에 따라 적합한 알고리즘 선택
   - 완전 관찰 가능: Q-learning, SARSA, 정책 그래디언트 등
   - 부분 관찰 가능: 믿음 상태 기반 알고리즘, 순환 신경망 정책 등

## 2.6 Markov 모델의 한계와 확장

### 2.6.1 한계점
- **Markov 가정의 현실성**: 실제 환경에서는 완벽한 Markov 속성이 성립하지 않을 수 있음
- **차원의 저주**: 상태 및 행동 공간이 커질수록 계산 복잡성 급증
- **부분 관찰성 문제**: 실제 환경에서는 완전한 상태 관찰이 어려움

### 2.6.2 확장 및 해결책
- **함수 근사**: 신경망을 사용하여 큰 상태 공간 처리
- **메모리 기반 접근법**: 순환 신경망(RNN)이나 LSTM을 사용하여 부분 관찰성 문제 해결
- **계층적 모델**: 복잡한 문제를 하위 문제로 분해하는 계층적 접근법
- **모델 기반 강화학습**: 환경 모델을 직접 학습하여 샘플 효율성 개선

## 2.7 Markov 모델의 분석 기법

### 2.7.1 정상 상태 분포(Stationary Distribution)
장기간 실행 후 각 상태에 있을 확률을 나타내는 분포로, 다음 방정식으로 표현됩니다:
$$\pi = \pi P$$
여기서 π는 정상 상태 확률 벡터, P는 전이 행렬입니다.

### 2.7.2 흡수 상태 분석(Absorbing State Analysis)
시스템이 특정 상태에 도달한 후 그 상태를 벗어나지 않는 경우, 이 상태를 흡수 상태라고 합니다. 흡수 Markov 체인의 분석은 다음을 포함합니다:
- 흡수 전까지 평균 시간
- 각 흡수 상태에 도달할 확률

### 2.7.3 뒤섞임 시간(Mixing Time)
Markov 체인이 정상 상태 분포에 얼마나 빨리 수렴하는지를 측정하는 시간입니다.

---

Markov 모델은 확률적 과정을 모델링하는 강력한 도구로, 강화학습의 이론적 기반을 형성합니다. 상태 전이의 확률적 특성과 Markov 속성을 이해하면 강화학습 알고리즘의 작동 원리와 적용 가능성을 더 깊이 이해할 수 있습니다.  
  

# 3. 강화학습 알고리즘 계통도

강화학습 알고리즘은 다양한 접근 방식과 특성에 따라 체계적으로 분류할 수 있습니다. 이 계통도는 주요 알고리즘 계열과 그 특징, 발전 과정을 정리합니다.

## 3.1 기본 프레임워크와 접근법

### 3.1.1 MDP (Markov Decision Process)

- **정의**: 강화학습의 이론적 토대로, 에이전트가 환경에서 보상을 최대화하기 위한 행동 선택 문제를 수학적으로 모델링
- **구성요소**: 
  - 상태 공간 (S)
  - 행동 공간 (A)
  - 상태 전이 확률 함수 P(s'|s,a)
  - 보상 함수 R(s,a,s')
  - 할인 인자 γ
- **중요성**: 대부분의 강화학습 알고리즘은 MDP 프레임워크를 기반으로 설계됨

### 3.1.2 접근 방식에 따른 분류

#### Model-Based vs Model-Free
- **Model-Based**: 환경 모델(전이 함수와 보상 함수)을 직접 학습하거나 사용
  - 장점: 샘플 효율성, 계획 가능
  - 단점: 모델 오류에 취약, 복잡한 환경에서 모델 구축 어려움
  - 예: Dyna-Q, MBRL (Model-Based RL)

- **Model-Free**: 환경 모델 없이 직접 경험으로부터 학습
  - 장점: 구현 단순, 복잡한 환경에 적용 용이
  - 단점: 샘플 비효율적
  - 예: Q-learning, SARSA, 정책 그래디언트

#### On-Policy vs Off-Policy
- **On-Policy**: 현재 학습 중인 정책과 동일한 정책으로 데이터 수집
  - 장점: 안정적 학습, 현재 정책의 성능 직접 평가
  - 단점: 데이터 효율성 낮음
  - 예: SARSA, PPO

- **Off-Policy**: 학습 중인 정책과 다른 정책(행동 정책)으로 데이터 수집
  - 장점: 데이터 재사용 가능, 탐험-활용 분리
  - 단점: 분포 불일치로 학습 불안정 가능성
  - 예: Q-learning, DQN, SAC

## 3.2 주요 알고리즘 계열

### 3.2.1 DP (Dynamic Programming)
- **특징**: 
  - 완전한 환경 모델(전이 확률, 보상 함수)을 알고 있다고 가정
  - 벨만 방정식의 반복적 계산을 통한 최적 정책 도출
- **주요 방법**:
  - 정책 평가(Policy Evaluation)
  - 정책 개선(Policy Improvement)
  - 정책 반복(Policy Iteration)
  - 가치 반복(Value Iteration)
- **한계**: 계산 복잡성, 대규모 상태 공간에 비실용적

### 3.2.2 몬테카를로 방식 (Monte Carlo Methods)
- **특징**:
  - 완전한 에피소드 경험을 통한 기대 반환값 추정
  - 환경 모델 불필요, 실제 경험에 기반
- **작동 원리**: 
  - 에피소드를 완료하고 실제 반환값을 관찰
  - 상태 또는 상태-행동 쌍의 가치를 평균 반환값으로 업데이트
- **장점**: 불편향 추정, 모델 오류 없음
- **단점**: 에피소드가 끝날 때까지 기다려야 함, 비효율적

### 3.2.3 TD 학습 (Temporal Difference Learning)
- **특징**:
  - 몬테카를로와 DP의 장점 결합
  - 부트스트래핑: 다른 추정값을 사용하여 현재 추정값 업데이트
- **업데이트 규칙**:
  ```
  V(s_t) ← V(s_t) + α[r_t+1 + γV(s_t+1) - V(s_t)]
  ```
- **장점**: 
  - 온라인 학습 가능
  - 에피소드 완료 전 학습 가능
  - 분산 감소
- **단점**: 
  - 초기 추정값에 편향 가능성
  - 추정값 간 오류 전파

### 3.2.4 가치 기반 방법 (Value-Based Methods)
- **핵심 개념**: 
  - 행동의 가치를 평가하여 최적 행동 선택
  - 정책은 가치 함수로부터 암시적으로 도출
- **주요 알고리즘**:
  - **SARSA** (State-Action-Reward-State-Action): 
    - 특징: On-policy TD 학습
    - 업데이트: Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]
    - 실제 선택한 다음 행동(a')의 Q값 사용
  
  - **Q-Learning**: 
    - 특징: Off-policy TD 학습
    - 업데이트: Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]
    - 가능한 모든 행동 중 최대 Q값 사용
  
  - **DQN** (Deep Q-Network): 
    - 특징: 딥러닝으로 Q함수 근사, 경험 리플레이, 타겟 네트워크
    - 혁신: 고차원 환경(Atari 게임)에서 인간 수준 성능 달성
    - 확장: Double DQN, Dueling DQN, Rainbow

### 3.2.5 정책 기반 방법 (Policy-Based Methods)
- **핵심 개념**: 
  - 정책을 직접 매개변수화하고 최적화
  - 상태에서 행동 확률 분포를 명시적으로 학습
- **주요 알고리즘**:
  - **REINFORCE**:
    - 특징: Monte Carlo 정책 그래디언트, 높은 분산
    - 업데이트: θ ← θ + α∇_θ log π_θ(a|s)G_t
  
  - **Actor-Critic**:
    - 특징: 정책(Actor)과 가치 함수(Critic) 동시 학습
    - 업데이트: θ ← θ + α∇_θ log π_θ(a|s)A(s,a)
    - A(s,a)는 Advantage 함수: 평균보다 얼마나 좋은지 평가
  
  - **A2C/A3C** (Advantage Actor-Critic/Asynchronous A2C):
    - 특징: 병렬 학습, 안정성 향상
  
  - **PPO** (Proximal Policy Optimization):
    - 특징: 클리핑을 통한 정책 업데이트 제한, 안정적 학습
    - 현업에서 널리 사용되는 알고리즘
  
  - **TRPO** (Trust Region Policy Optimization):
    - 특징: KL 발산 제약을 통한 정책 업데이트 안정화
    - 계산 복잡성 높음

### 3.2.6 모델 기반 RL (Model-Based RL)
- **핵심 개념**: 
  - 환경 모델을 명시적으로 학습하고 활용
  - 계획(Planning)을 통한 정책 개선
- **주요 알고리즘**:
  - **Dyna-Q**:
    - 특징: 실제 경험과 모델 기반 시뮬레이션 병행
  
  - **PILCO** (Probabilistic Inference for Learning Control):
    - 특징: 가우시안 프로세스로 환경 불확실성 모델링
  
  - **MuZero**:
    - 특징: 명시적 모델 없이 계획에 필요한 표현만 학습
    - 체스, 바둑, Atari 게임에서 최고 성능

### 3.2.7 하이브리드 및 특수 알고리즘
- **MaxEnt RL** (최대 엔트로피 강화학습):
  - 특징: 엔트로피 최대화를 통한 탐험 장려
  - 예: SAC(Soft Actor-Critic), 높은 샘플 효율성
  
- **분산 강화학습**:
  - 특징: 병렬 환경에서 학습 가속화
  - 예: A3C, IMPALA, Ape-X
  
- **다중 에이전트 RL**:
  - 특징: 여러 에이전트가 상호작용하는 환경에서 학습
  - 예: MADDPG, QMIX, MAPPO

## 3.3 알고리즘 간 관계 및 진화

### 3.3.1 알고리즘 진화 계보

```
DP (Dynamic Programming)
    ↓
Monte Carlo Methods       TD Learning
    ↓                        ↓
    ←---------+---------------→
               ↓
         Q-Learning  ←-----→  SARSA
               ↓
              DQN
               ↓
      Double DQN, Dueling DQN
               ↓
            Rainbow
               
Policy Gradient Methods
    ↓
REINFORCE
    ↓
Actor-Critic
    ↓
A2C/A3C  ←-----→  TRPO  →  PPO
                    ↓
                   SAC
```

### 3.3.2 알고리즘 성능 특성 비교

| 알고리즘      | 샘플 효율성 | 안정성  | 탐험 효율 | 구현 복잡성 | 연속 행동 |
|--------------|-----------|--------|----------|------------|----------|
| Q-Learning   | 중        | 중     | 중       | 낮음       | 어려움    |
| SARSA        | 중        | 높음   | 낮음     | 낮음       | 어려움    |
| DQN          | 낮음      | 중     | 중       | 중         | 어려움    |
| REINFORCE    | 매우 낮음  | 낮음   | 높음     | 낮음       | 가능      |
| A2C/A3C      | 중        | 중     | 중       | 중         | 가능      |
| PPO          | 중-높음   | 높음   | 중       | 중         | 가능      |
| TRPO         | 중-높음   | 매우 높음| 중      | 높음       | 가능      |
| SAC          | 높음      | 높음   | 높음     | 중-높음    | 가능      |
| Model-Based  | 매우 높음  | 모델 의존| 모델 의존| 매우 높음  | 가능      |

## 3.4 최신 동향 및 발전 방향

### 3.4.1 딥 강화학습의 발전
- **표현 학습**: 복잡한 환경에서 유용한 특징 자동 추출
- **사전 학습 및 전이 학습**: 기존 지식 활용 및 일반화 능력 향상
- **자기 지도 학습**: 레이블 없는 데이터에서 학습

### 3.4.2 분산 및 병렬 학습
- 병렬 환경에서 동시 학습을 통한 샘플 효율성 극대화
- 하드웨어 자원 활용 최적화

### 3.4.3 실제 환경 적용 문제 해결
- **안전 강화학습**: 위험한 행동 방지
- **오프라인 강화학습**: 기존 데이터셋만으로 학습
- **멀티태스크 및 메타 학습**: 다양한 작업 간 지식 전이

### 3.4.4 알고리즘 하이브리드화
- 각 알고리즘의 장점을 결합한 새로운 접근법 개발
- 특정 도메인에 최적화된 알고리즘 설계

---

강화학습 알고리즘은 계속 발전하고 있으며, 각 알고리즘은 특정 문제 도메인이나 상황에 더 적합할 수 있습니다. 알고리즘 선택 시에는 문제의 특성, 환경의 복잡성, 샘플 효율성 요구 사항 등을 고려하는 것이 중요합니다.

## 🔷 TD 기반 분류

### 📌 A. **가치 기반 강화학습 (Value-Based Reinforcement Learning)**
에이전트가 환경과 상호작용하며 **"행동의 가치를 학습"**하는 방식  
핵심은 "어떤 행동이 좋은가?"를 수치로 평가해서, 최적의 행동을 선택하는 것  
- 가치 함수(Value Function)를 학습하여 행동 결정
- **주요 알고리즘**:
  - SARSA → Deep SARSA
  - Q-Learning → DQN ( Neural ODE와 연결가능성 )
- **특징**:
  - 정책을 명시적으로 표현하지 않음
  - 가치 함수 기반 행동 선택

### 📌 B. **정책 기반 강화학습 (Policy-Based Reinforcement Learning)**
정책 기반 강화학습은 에이전트가 직접 정책(Policy)을 학습하여  
상태에서 어떤 행동을 할지를 확률적으로 결정하는 방식
- 직접 정책(π)을 최적화
- **주요 알고리즘**:
  - REINFORCE
  - A2C (Advantage Actor-Critic)
  - Continuous A2C
  - A3C (Asynchronous Advantage Actor-Critic)
  - PPO (Proximal Policy Optimization)
  - DDPG, TD3, SAC ...
- **특징**:
  - 연속 행동 공간에 유리
  - 안정적인 수렴 성질
  - 가치 함수와 정책을 함께 사용하는 Actor-Critic 구조

# 4. 강화학습: 탐험(Exploration), 이용(Exploitation), ε-Greedy

강화학습에서 가장 중요한 딜레마 중 하나는 '탐험(Exploration)'과 '이용(Exploitation)' 사이의 균형을 맞추는 문제입니다. 이 문제는 에이전트가 학습 과정에서 언제 새로운 경험을 탐색하고, 언제 현재 알고 있는 최적의 전략을 활용할지 결정하는 것과 관련되어 있습니다.

## 4.1 탐험(Exploration)

### 정의와 목표
탐험은 에이전트가 아직 경험하지 못한 상태와 행동을 시도하여 환경에 대한 더 많은 정보를 수집하는 과정입니다. 이는 장기적으로 더 나은 전략을 발견하기 위한 투자로 볼 수 있습니다.

### 특징
- **무작위성**: 현재 지식에 의존하지 않고 무작위로 행동을 선택하는 경향이 있음
- **불확실성 감소**: 환경에 대한 이해를 넓히고 불확실성을 줄이는 데 목적이 있음
- **장기적 관점**: 즉각적인 보상보다 미래의 더 큰 보상 가능성을 위해 투자

### 장점
- 최적 정책을 찾을 가능성 증가
- 지역 최적화(local optima)에 빠지는 것을 방지
- 환경 변화에 더 잘 적응할 수 있음

### 단점
- 단기적으로는 비효율적일 수 있음
- 불필요한 경험에 시간과 자원을 낭비할 가능성
- 위험한 상태를 탐색할 수 있음 (특히 실제 환경에서)

## 4.2 이용(Exploitation)

### 정의와 목표
이용은 에이전트가 현재까지 학습한 지식을 바탕으로 가장 높은 보상을 얻을 것으로 예상되는 행동을 선택하는 과정입니다. 즉, 현재 알고 있는 최선의 전략을 활용하는 것입니다.

### 특징
- **탐욕적 선택**: 예상 보상을 최대화하는 행동을 선택
- **현재 지식 활용**: 지금까지 학습한 가치 함수나 정책에 기반한 의사결정
- **단기적 최적화**: 즉각적인 보상을 극대화하는 데 중점

### 장점
- 알려진 좋은 전략을 효율적으로 활용
- 안정적인 성능 보장
- 리소스 효율성: 이미 검증된 경로에 집중

### 단점
- 미발견된 더 좋은 전략을 놓칠 수 있음
- 지역 최적화에 쉽게 수렴
- 환경 변화에 적응하기 어려움

## 4.3 ε-Greedy 알고리즘

ε-Greedy는 탐험과 이용 사이의 균형을 맞추기 위한 가장 간단하면서도 효과적인 방법 중 하나입니다.

### 원리
- 확률 ε(epsilon)로 무작위 행동을 선택 (탐험)
- 확률 (1-ε)로 현재 최적이라고 생각되는 행동 선택 (이용)

### 수학적 표현
선택 확률 P(a|s):
- P(a|s) = ε/|A| + (1-ε) · I(a = argmax Q(s,a'))
- 여기서 |A|는 가능한 행동의 수, I는 지시 함수(indicator function)

### 구현 예시 (Python)
```python
def epsilon_greedy_policy(Q, state, epsilon, n_actions):
    if random.random() < epsilon:
        # 탐험: 무작위 행동 선택
        return random.randint(0, n_actions - 1)
    else:
        # 이용: 최대 Q값을 가진 행동 선택
        return np.argmax(Q[state, :])
```

### ε 값의 설정과 조정
- **높은 ε 값(예: 0.9)**: 학습 초기에 탐험에 중점
- **낮은 ε 값(예: 0.1)**: 학습 후반에 이용에 중점
- **감소하는 ε 값**: 시간이 지남에 따라 점진적으로 ε 값을 줄여나가는 전략
  ```python
  epsilon = max(epsilon_min, epsilon * epsilon_decay)
  ```

## 4.4 탐험과 이용의 비교

| 특성 | 탐험(Exploration) | 이용(Exploitation) |
|------|-------------------|-------------------|
| 목표 | 환경에 대한 정보 수집 | 보상 최대화 |
| 시간 관점 | 장기적 투자 | 단기적 최적화 |
| 리스크 | 높음 (불확실한 결과) | 낮음 (예측 가능한 결과) |
| 학습 기여 | 새로운 지식 획득 | 기존 지식 활용 |
| 변화 대응 | 적응력 높음 | 안정성 높음 |
| 결과 분산 | 높음 | 낮음 |

## 4.5 탐험 전략의 발전

### 4.5.1 단순 전략
- **ε-Greedy**: 일정 확률로 무작위 행동 선택
- **Boltzmann Exploration**: Q값에 따른 확률적 행동 선택 (softmax)
  ```
  P(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)
  ```
  여기서 τ는 온도 파라미터로, 높을수록 행동 선택이 더 무작위적임

### 4.5.2 컨텍스트 기반 전략
- **UCB (Upper Confidence Bound)**: 불확실성이 높은 행동에 탐험 보너스 부여
  ```
  a = argmax[ Q(s,a) + c√(ln(t)/N(s,a)) ]
  ```
  N(s,a)는 행동 a를 상태 s에서 선택한 횟수, t는 총 타임스텝

- **Optimistic Initialization**: Q값을 초기에 높게 설정해 자연스러운 탐험 유도

### 4.5.3 고급 탐험 방법
- **Intrinsic Motivation**: 호기심이나 놀라움을 내부 보상으로 부여
- **Count-Based Exploration**: 방문 빈도가 낮은 상태에 보상 부여
- **Parameter Space Noise**: 정책 파라미터에 노이즈 추가
- **Noisy Networks**: 신경망 가중치에 학습 가능한 노이즈 추가

## 4.6 실제 응용 사례

### 4.6.1 강화학습 알고리즘에서의 탐험-이용 전략
- **DQN**: ε-greedy 전략과 경험 리플레이 결합
- **DDPG**: 행동에 노이즈 추가 (주로 OU 노이즈)
- **PPO**: 클리핑된 서러게이트 목적함수와 엔트로피 보너스
- **SAC**: 엔트로피 최대화를 통한 자동 탐험

### 4.6.2 실제 문제 도메인 특성에 따른 전략
- **게임 AI**: 초기에 높은 탐험률, 점진적 감소
- **로봇 제어**: 안전 제약 조건 하에서의 제한된 탐험
- **추천 시스템**: 사용자 피드백 기반 컨텍스트 탐험
- **의료 의사결정**: 위험 관리와 함께하는 조심스러운 탐험

## 4.7 방법론적 고려사항

### 4.7.1 문제 특성에 따른 탐험-이용 균형
- **환경의 변동성**: 환경이 빠르게 변하면 탐험 비중 증가
- **작업의 위험성**: 실패 비용이 클수록 이용 비중 증가
- **학습 단계**: 초기에는 탐험, 후기에는 이용 비중 증가
- **차원의 저주**: 상태 공간이 클수록 효율적 탐험 방법 필요

### 4.7.2 평가 지표
- **누적 보상**: 전체 학습 과정의 총 보상
- **수렴 속도**: 최적 정책에 도달하는 데 필요한 시간
- **최종 성능**: 학습 완료 후 정책의 품질
- **안정성**: 학습 과정의 변동성

---

탐험과 이용의 균형은 강화학습의 핵심 과제 중 하나로, 알고리즘과 문제 도메인에 따라 다양한 전략이 개발되어 왔습니다. 단순한 ε-greedy부터 최신 내재적 동기 부여 방법까지, 적절한 탐험-이용 전략을 선택하고 조정하는 것은 강화학습 시스템의 성능을 결정짓는 중요한 요소입니다.

# 5. Markov Decision Process (MDP)

Markov Decision Process(MDP)는 강화학습의 수학적 기반으로, 순차적 의사결정 문제를 모델링하는 프레임워크입니다. 이 모델은 에이전트가 환경과 상호작용하는 과정을 확률적으로 표현하며, 대부분의 강화학습 알고리즘은 MDP를 기반으로 설계됩니다.

## 5.1 MDP의 정의와 구성요소

MDP는 다음 5가지 요소로 구성된 튜플 $(S, A, P, R, \gamma)$로 정의됩니다:

### 5.1.1 상태 공간 (State Space, $S$)
- 환경이 취할 수 있는 모든 가능한 상태의 집합
- 이산적(discrete) 또는 연속적(continuous) 상태 공간으로 구분
- 예: 체스에서의 보드 배치, 로봇의 위치와 속도, 주식 시장의 상태 등

### 5.1.2 행동 공간 (Action Space, $A$)
- 에이전트가 선택할 수 있는 모든 가능한 행동의 집합
- 이산적 또는 연속적 행동 공간으로 구분
- 예: 게임에서의 방향 키 입력, 로봇 관절의 토크값, 주식 매수/매도 결정 등

### 5.1.3 상태 전이 확률 (Transition Probability, $P$)
- 현재 상태와 행동이 주어졌을 때 다음 상태로 전이될 확률을 정의하는 함수
- $P(s'|s,a)$: 상태 $s$에서 행동 $a$를 취했을 때 상태 $s'$로 전이될 확률
- 환경의 역학(dynamics)을 확률적으로 모델링

### 5.1.4 보상 함수 (Reward Function, $R$)
- 에이전트가 특정 상태에서 특정 행동을 취했을 때 받는 즉각적인 보상을 정의
- $R(s,a,s')$: 상태 $s$에서 행동 $a$를 취하고 상태 $s'$로 전이했을 때 받는 보상
- 에이전트의 목표(goal)를 수치적으로 표현

### 5.1.5 할인 인자 (Discount Factor, $\gamma$)
- 미래 보상의 현재 가치를 계산하는 데 사용되는 파라미터 (0 ≤ $\gamma$ ≤ 1)
- 작은 $\gamma$ 값: 가까운 미래의 보상 중시 (근시안적 결정)
- 큰 $\gamma$ 값: 먼 미래의 보상까지 고려 (장기적 결정)

## 5.2 MDP의 핵심 특성: Markov 속성

MDP의 가장 중요한 특성은 Markov 속성(Markov property)입니다. 이는 다음 상태가 오직 현재 상태와 현재 행동에만 의존하고, 과거 상태와 행동의 이력에는 의존하지 않는다는 것을 의미합니다.

$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1}|s_t, a_t)$$

### 5.2.1 Markov 속성의 의미
- **상태 표현의 충분성**: 현재 상태는 미래 예측에 필요한 모든 정보를 포함
- **차원 축소**: 과거 이력 전체가 아닌 현재 상태만 고려하면 되므로 계산 복잡성 감소
- **효율적 학습**: 현재 상태만으로 의사결정이 가능하므로 학습 효율성 증가

### 5.2.2 정책의 Markov 속성
정책(policy) $\pi$는 각 상태에서 어떤 행동을 취할지 결정하는 규칙입니다. Markov 속성에 따라 정책은 현재 상태에만 의존합니다:

$$\pi(a_t|s_t, a_{t-1}, s_{t-1}, ...) = \pi(a_t|s_t)$$

## 5.3 MDP에서의 평가 지표: 가치 함수

MDP에서는 두 가지 주요 가치 함수를 통해 정책의 성능을 평가합니다:

### 5.3.1 상태 가치 함수 (State Value Function, $V^\pi(s)$)
- 정책 $\pi$를 따를 때 상태 $s$에서 시작하여 기대되는 누적 할인 보상
- 정의: $V^\pi(s) = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$
- 의미: 상태 $s$가 얼마나 '좋은' 상태인지 평가

### 5.3.2 행동 가치 함수 (Action Value Function, $Q^\pi(s,a)$)
- 정책 $\pi$를 따를 때 상태 $s$에서 행동 $a$를 취한 후 기대되는 누적 할인 보상
- 정의: $Q^\pi(s,a) = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]$
- 의미: 상태 $s$에서 행동 $a$를 선택하는 것이 얼마나 '좋은' 선택인지 평가

### 5.3.3 상태 가치와 행동 가치의 관계
상태 가치와 행동 가치는 다음과 같은 관계가 있습니다:

$$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$$

즉, 상태 가치는 해당 상태에서 가능한 모든 행동의 가치를 정책에 따른 확률로 가중 평균한 값입니다.

## 5.4 MDP 문제 해결을 위한 핵심 방정식: 벨만 방정식

벨만 방정식(Bellman Equation)은 가치 함수의 재귀적 관계를 표현하는 핵심 방정식입니다.

### 5.4.1 벨만 기대 방정식 (Bellman Expectation Equation)
정책 $\pi$에 대한 상태 가치 함수와 행동 가치 함수의 재귀적 관계를 나타냅니다:

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]$$

$$Q^\pi(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$$

### 5.4.2 벨만 최적 방정식 (Bellman Optimality Equation)
최적 가치 함수에 대한 재귀적 관계를 나타냅니다:

$$V^*(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$$

$$Q^*(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q^*(s',a')]$$

## 5.5 MDP 문제의 목표: 최적 정책 찾기

MDP 문제의 궁극적인 목표는 기대 누적 보상을 최대화하는 최적 정책 $\pi^*$를 찾는 것입니다.

### 5.5.1 최적 정책의 특성

최적 정책 $\pi^*$는 모든 상태에서 최적 행동(최대 가치를 갖는 행동)을 선택합니다:

$$\pi^*(s) = \arg\max_a Q^*(s,a)$$

이 정책은 모든 상태에서 최대 가치를 달성합니다:

$$V^{\pi^*}(s) = V^*(s) = \max_\pi V^\pi(s)$$

### 5.5.2 최적 정책의 결정론적 특성

흥미롭게도, 결정론적 환경에서는 최적 정책이 항상 결정론적입니다. 즉, 각 상태에서 확률적으로 행동을 선택하는 것보다, 항상 가장 가치가 높은 하나의 행동만 선택하는 것이 최적입니다.

## 5.6 MDP 해결 방법

MDP 문제를 해결하는 방법은 크게 세 가지로 나눌 수 있습니다:

### 5.6.1 동적 계획법 (Dynamic Programming)
- 환경 모델(전이 확률, 보상 함수)을 알고 있을 때 사용
- 가치 반복(Value Iteration)
- 정책 반복(Policy Iteration)

### 5.6.2 모델 없는 방법 (Model-Free Methods)
- 환경 모델 없이 경험을 통해 직접 학습
- 몬테카를로(Monte Carlo) 방법
- 시간차(Temporal Difference) 학습 (e.g., Q-learning, SARSA)

### 5.6.3 함수 근사 방법 (Function Approximation)
- 대규모 또는 연속적인 상태 공간에서 가치 함수나 정책을 근사
- 선형 함수 근사
- 신경망 기반 근사 (딥 강화학습)

## 5.7 MDP의 변형과 확장

### 5.7.1 부분 관찰 가능 MDP (POMDP)
- 에이전트가 환경의 상태를 직접 관찰할 수 없고, 관측을 통해 간접적으로 추론해야 하는 상황
- 믿음 상태(belief state)를 통해 의사결정

### 5.7.2 평균 보상 MDP (Average-Reward MDP)
- 할인 인자를 사용하지 않고 장기적인 평균 보상을 최대화하는 방법
- 무한 지속 작업에 적합

### 5.7.3 계층적 MDP (Hierarchical MDP)
- 복잡한 작업을 계층적으로 분해하여 해결
- 추상화 수준에 따라 의사결정

## 5.8 MDP의 한계점

### 5.8.1 차원의 저주 (Curse of Dimensionality)
- 상태와 행동 공간이 커질수록 계산 복잡성 급증
- 함수 근사를 통한 일반화가 필요

### 5.8.2 모델 불확실성 (Model Uncertainty)
- 실제 환경의 역학을 정확히 모델링하기 어려움
- 모델 기반 학습의 성능 제한

### 5.8.3 Markov 가정의 제한
- 실제 환경에서는 완벽한 Markov 가정이 성립하지 않을 수 있음
- 상태 표현의 불완전성

## 5.9 MDP 연구의 최신 동향

### 5.9.1 리스크 민감 MDP (Risk-Sensitive MDP)
- 기대 보상뿐 아니라 리스크(보상 분산)도 고려
- 금융, 의료 등 위험 관리가 중요한 분야에 적용

### 5.9.2 적대적 MDP (Adversarial MDP)
- 환경이 에이전트에 적대적으로 행동할 수 있는 상황을 고려
- 보안, 로버스트 제어 등에 활용

### 5.9.3 다중 목표 MDP (Multi-Objective MDP)
- 단일 보상이 아닌 여러 목표를 동시에 최적화
- 실제 응용에서 다양한 제약 조건 반영

---

MDP는 강화학습의 이론적 기반을 제공하며, 순차적 의사결정 문제를 모델링하는 강력한 프레임워크입니다. 그 단순함에도 불구하고 복잡한 실제 문제를 추상화하고 해결하는 데 매우 유용한 도구입니다. 현대 강화학습 알고리즘의 대부분이 MDP를 기반으로 설계되었으며, MDP의 이해는 강화학습 이론과 알고리즘 개발에 필수적입니다.  

# 6-1. Ballman Equation 
## about State value function & Action value function

## ✅ 개념 정리

### 🔹 State Value Function (상태 가치 함수)

- **정의**:  
  현재 상태에서 앞으로 기대되는 누적 보상 (Return)의 기대값
- **의미**:  
  지금 상태가 얼마나 "좋은지"를 평가하는 함수  
- **수식**:
  \[
  $V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]$
  \]

- **직관**:  
  큰 값을 가질수록 좋은 상태다 (좋은 정책이면 좋을 상태로 이동해야 함)

---

### 🔹 Action Value Function (행동 가치 함수)

- **정의**:  
  특정 상태에서 특정 행동을 했을 때 앞으로 기대되는 누적 보상의 기대값
- **의미**:  
  이 행동을 지금 이 상태에서 선택했을 때 얼마나 좋은가?
- **수식**:
  \[
 $Q^\pi(s, a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ]$
  \]

---

## 🎯 Return 정의

\[
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$
\]

- \( $\gamma$ \): discount factor (0 < γ ≤ 1)  
- 미래 보상의 현재 가치 반영

---

## 📐 수식적 정의

1. 상태 가치 함수 ( State value function ):

\[
$V(s_t) = \int_{a_t}^\infty G_t(a_t, s_{t+1}, a_{t+1}, \dots \mid s_t) \, da_t$
\]

2. 행동 가치 함수 ( Action value function ):

\[
$Q(s_t, a_t) = \int_{s_{t+1}}^\infty G_t(s_{t+1}, a_{t+1}, s_{t+2}, a_{t+2}, \dots \mid s_t, a_t) \, ds_{t+1}$
\]

> 각 수식은 기대값 관점에서 **미래의 전체 trajectory**를 고려하는 연속 확률 표현입니다.

---

## 🏁 Optimal Policy란?

- 최적 정책은 **상태 가치 함수**를 **최대한으로 만드는 정책**입니다.
- 수식적으로는:

\[
$\pi^* = \arg\max_\pi V^\pi(s)$
\]

또는 아래처럼 **상태-행동 전이 확률 시퀀스를 통해 최대화하는** 정책이 최적 정책이 됩니다:

\[
$\pi^* \propto \max \left( 
P(a_t, s_t),\ 
P(a_{t+1}, s_{t+1}),\ 
\dots,\ 
P(a_\infty, s_\infty)
\right)$
\]

---

## 🧠 요약

| 항목 | 의미 | 수식 | 설명 |
|------|------|------|------|
| \( $V(s)$ \) | 상태 가치 | \( $\mathbb{E}[G_t \mid s] $\) | 상태만 보고 가치 판단 |
| \( $Q(s, a)$ \) | 행동 가치 | \( $\mathbb{E}[G_t \mid s, a] $\) | 상태 + 행동 조합으로 가치 판단 |
| \($\pi^*$\) | 최적 정책 | \( $\arg\max_\pi V^\pi(s) $\) | 가장 좋은 상태를 만드는 정책 |  
  
    

# 6-1 + Specification: Bellman Equation Detailed Analysis

## Return 정의

\[
$G_t \triangleq R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots$
\]

### 1. 상태 가치 함수 \(V(S_t)\)

상태 가치 함수 $V(S_t)$는 현재 상태 $S_t$에서 시작하는 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t P(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

여기서 $P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t)$는 상태 전이 확률이며, $P(a_t \mid S_t)$는 주어진 상태에서의 정책에 의해 선택된 행동의 확률입니다.

## 과정 분석

### 상태 전이 및 반환값 계산

\[
$\int_{a_t, S_{t+1}} G_t P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 $Q(S_t, a_t)$를 사용하여 다음과 같이 재표현할 수 있습니다:

\[
$\int_{a_t} Q(S_t, a_t) P(a_t \mid S_t) da_t$
\]

### 행동 가치 함수 계산

\[
$\int_{a_t, S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_t, S_{t+1} \mid S_t) da_t, S_{t+1}$
\]

이 과정을 통해 $V(S_t)$에서 $V(S_{t+1})$로의 전이가 이루어지며, 상태 간의 전이 확률 $P(S_{t+1} \mid S_t, a_t)$와 정책 $P(a_t \mid S_t)$을 반영합니다.

## 행동 가치 함수 \( $Q(S_t, a_t)$ \) 계산

행동 가치 함수 $Q(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 기대 반환값을 계산합니다.

\[
$Q(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

### 계산 과정

1. **첫 번째 단계의 계산:**

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_{t+1}, \ldots \mid S_{t+1}) dS_{t+1} : a_\infty \Rightarrow V(S_{t+1})$
\]

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(S_{t+1} \mid S_t, a_t) dS_{t+1}$
\]

2. **두 번째 단계의 계산:**

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma G_{t+1}) P(S_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) dS_{t+2} : a_\infty \Rightarrow Q(S_{t+1}, a_{t+1})$
\]

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma Q(S_{t+1}, a_{t+1})) P(S_{t+1}, a_{t+1} \mid S_t, a_t) da_{t+1}, S_{t+1}$
\]

이러한 수식적 분석을 통해 벨만 방정식의 재귀적 특성과 가치 함수 간의 관계를 정의할 수 있습니다.

# Optimal Policy Analysis

## 상태 가치 함수 \( $V(S_t)$ \)

상태 가치 함수 $V(S_t)$는 현재 상태에서 시작하여 취할 수 있는 모든 행동의 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

이는 최적의 행동 가치 함수 $Q^*(S_t, a_t)$를 사용하여 다음과 같이 표현됩니다:

\[
$V(S_t) = \int_{a_t} Q^*(S_t, a_t) P(a_t \mid S_t) da_t$
\]

여기서 $argmax P(a_t \mid S_t)$는 주어진 상태에서 최적의 행동을 선택합니다.

## 최적 행동 가치 함수 \( $Q^*(S_t, a_t)$ \)

최적 행동 가치 함수 $Q^*(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 최대 기대 반환값을 계산합니다.

\[
$Q^*(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 모든 가능한 이후 상태의 시퀀스를 고려하여 계산됩니다:

\[
$Q^*(S_t, a_t) = P(a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_{t}, a_t) P(S_{t+1} \mid S_t, a_t)$
\]

\[
= $P(S_{t+2}, a_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) P^*(a_{t+1} \mid S_{t+1})$
\]

\[
$...$
\]

## 7. Q - value 는 어떻게 찾을까? ( 강화학습에서의 Q 값 추정 )

# Monte-Carlo (MC) vs Temporal Difference (TD)

강화학습에서 $Q$-value를 추정하는 대표적인 두 가지 방법인 **Monte-Carlo(MC)** 방식과 **Temporal Difference(TD)** 방식은 서로 다른 철학을 갖고 접근

---

## 🔹 Monte-Carlo 방법 (MC)

- **정의**: 전체 에피소드가 끝난 후, 실제 관측된 누적 보상 $G_t$를 평균하여 $Q$ 값을 추정
- **수식**:
  $Q(s_t, a_t) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$
- **특징**:
  - 전체 trajectory(궤적)를 따라가며 실제 리턴 $G_t$를 계산
  - 모델 없이 순수한 경험 기반 학습
- **장점**:
  - 불편추정(unbiased estimate)
  - 간단하고 직관적
- **단점**:
  - 에피소드가 끝날 때까지 기다려야 함 (느림)
  - 온라인 학습에 부적합

---

## 🔹 Temporal Difference 방법 (TD)
모델이 없거나(model-free), 모델 역학(model dynamics)을 미리 알 필요가 없는 비 에피소드 작업에도 적용 가능한 학습 알고리즘
- **정의**: 다음 상태의 $Q$ 값을 기반으로 한 단계씩 근사 추정
- **수식**:  

  $Q(s_t, a_t) \approx R_t + \gamma Q(s_{t+1}, a_{t+1})$
  또는 평균을 사용한 업데이트 방식:  

  $Q_N = \frac{1}{N} \sum_{i=1}^{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) \right)$
  그리고 점진적 업데이트:  
  
  $Q_N = Q_{N-1} + \frac{1}{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) - Q_{N-1} \right)$
- **특징**:
  - 리턴 전체를 기다리지 않고, 예측된 다음 $Q$ 값을 사용하여 즉시 업데이트
  - 학습 속도가 빠르고 실시간 학습에 적합
- **장점**:
  - 에피소드 종료를 기다릴 필요 없음 (빠름)
  - 온라인 학습 가능
- **단점**:
  - 추정값을 바탕으로 업데이트되므로 bias가 존재할 수 있음
  -  Monte Carlo 방법은 에피소드 환경에만 적용된다는 단점과 에피소드가 매우 길면 가치 함수를 계산하기 위해 오랜 시간이 걸림

---

## 📊 비교 정리

| 항목 | Monte-Carlo (MC) | Temporal Difference (TD) |
|------|------------------|---------------------------|
| 학습 시점 | 에피소드 종료 후 | 한 스텝마다 (온라인) |
| 보상 사용 | 실제 누적 보상 $G_t$ | 예상된 $Q(s_{t+1}, a_{t+1})$ |
| 추정 성질 | 불편추정 (Unbiased) | 편향추정 (Biased) |
| 데이터 기반 | 완전한 경험 | 예측 포함 |
| 장점 | 안정적, 직관적 | 빠름, 실시간 학습 가능 |
| 단점 | 느림, 오프라인 | 초기 불안정, 편향 가능성 |

---

## ✅ 결론

- Monte-Carlo는 **정확하지만 느리며**, 에피소드 단위로 학습합니다.
- Temporal Difference는 **빠르지만 추정 기반**으로 더 빠른 반응과 실시간 처리가 가능합니다.
- 실제 강화학습에서는 이 두 방식을 적절히 혼합한 **SARSA, Q-learning** 같은 알고리즘들이 널리 사용됩니다.

# 8. 강화학습 방법


강화학습은 에이전트가 환경과 상호작용하며 최적의 정책(policy)을 학습하는 방법이다. 이 학습 과정은 **정책 실행을 통한 샘플 생성**, **가치함수 추정**, **정책 개선**의 **3단계**를 반복하는 **iteration 구조**로 이루어진다.

---

## ✅ 강화학습의 3단계 반복 구조

### 1. 정책 실행을 통한 샘플 생성
에이전트가 현재의 정책을 바탕으로 환경에서 행동을 수행하며, 상태(state), 행동(action), 보상(reward), 다음 상태(next state) 등의 **경험 데이터(experience)** 또는 **전이(transition)** 를 수집한다. 이러한 (s, a, r, s') 튜플을 **샘플**이라고 부른다.

### 2. 가치 함수 추정
수집된 경험 샘플을 바탕으로 **가치 함수(value function)** 또는 **행동 가치 함수(action-value function, Q-function)** 를 추정한다. 이는 각 상태나 상태-행동 쌍에 대한 기대 보상을 평가하는 함수이다.

### 3. 정책 개선
추정된 가치 함수에 기반하여 정책을 수정하거나, 가치가 높은 행동을 선택하도록 정책을 조정한다. 이로써 더 나은 행동 선택이 가능해진다.

이 3단계를 **반복(iteration)** 하여 정책을 점점 최적화해간다.

---

## 🧮 강화학습의 수학적 목적

강화학습의 목적은 다음과 같이 수학적으로 정의된다:

\[
$\theta^* = \arg\max_{\theta} J(\theta)$
\]

\[
$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right]$
\]

- \( $J(\theta)$ \): 정책에 대한 기대 보상
- \( $r(x_t, u_t)$ \): 시간 t에서 상태 \(x_t\)에서 행동 \(u_t\)를 했을 때 받는 보상
- \( $\gamma$ \): 할인율 (discount factor)
- \( $p_\theta(\tau)$ \): 정책에 의해 생성된 trajectory \( \tau \)의 확률분포

이러한 수식을 통해 강화학습의 목표는 **기댓값이 최대가 되는 정책 파라미터 \($\theta$\)** 를 찾는 것이다.

---

## 🔁 Trajectory : 정책에 따라 에이전트가 환경과 상호작용하며 만들어낸 상태와 행동의 연속된 순서 표현

\[
$\tau = (x_0, u_0, x_1, u_1, ..., x_T, u_T)$
\]

\( $\tau$ \)는 정책 \( $\pi_\theta$ \) 에 의해 생성되는 상태-행동의 시퀀스이며, 일반적으로 신경망(neural network)을 통해 정책을 표현한다.

---

## 🤖 MDP 프레임워크에서의 상호작용 구조

Agent와 환경(Environment) 간 상호작용은 MDP(Markov Decision Process) 프레임워크로 표현할 수 있다.

- 에이전트는 상태 \(x_t\)를 관찰하고, 정책 \(\pi_\theta(u_t | x_t)\)에 따라 행동 \(u_t\)를 선택한다.
- 환경은 행동 \(u_t\)에 따라 보상 \(r_t\)와 다음 상태 \(x_{t+1}\)를 반환한다.
- 이러한 상호작용은 에피소드가 끝날 때까지 반복된다.

### 전체 반환 (Return):

\[
$G_0 = \sum_{t=0}^{T} \gamma^t r(x_t, u_t)$
\]

이는 시간 t=0부터 에피소드 종료 시점까지 받을 수 있는 **discounted reward의 총합**이다.

### 특정 시점 k부터의 반환:

\[
$G_t = \sum_{k=t}^{T} \gamma^{k-t} r(x_k, u_k)$
\]

이는 시간 t부터 받을 수 있는 누적 보상(return)을 의미한다.

---

## 📌 정리

- 강화학습은 반복적으로 정책을 개선하는 최적화 문제이다.
- 기대 보상을 최대화하는 방향으로 정책 파라미터를 조정한다.
- 대표적인 두 방법은 다음과 같다:
  - **Policy Gradient**: 정책을 직접 매개변수화하여 최적화 (예: Actor-Critic)
  - **Value-based**: 가치함수를 추정하여 간접적으로 정책 유도 (예: DQN)

## 9. 강화학습의 목적함수 및 정책 그래디언트 정리

강화학습에서 에이전트의 목표는 주어진 정책 $\pi_\theta$ 하에서 얻을 수 있는 기대 보상, 즉 목적함수 $J(\theta)$를 최대화하는 것이다.

---

#### 1. Trajectory의 확률 밀도 함수

Chain rule을 이용하여 trajectory $\tau$에 대한 확률 밀도 함수 $p_\theta(\tau)$는 아래와 같이 전개된다.

$p_\theta(\tau) = p(x_0, u_0, x_1, u_1, \dots, x_T, u_T)
= p(x_0)p_\theta(u_0, x_1, u_1, \dots, x_T, u_T \mid x_0)$

초기 상태 변수 $x_0$의 확률 밀도 함수는 정책 $\pi_\theta$와 무관하므로 $p(x_0)$로 표현한다. 위 식의 두 번째 줄에도 chain rule을 연달아 적용하면 다음과 같다.

$$\begin{align*}
p_\theta(\tau) &= p(x_0)p_\theta(u_0 \mid x_0)p(x_1 \mid x_0, u_0)p_\theta(u_1 \mid x_0, u_0, x_1) \\
&\quad \cdot p(x_2 \mid x_0, u_0, x_1, u_1)p_\theta(u_2 \mid x_0, \dots)p(x_3 \mid \dots) \dots
\end{align*}$$  


Markov 시퀀스 가정에 의해 아래와 같이 간단화할 수 있다:

$p_\theta(u_1 \mid x_0, u_0, x_1) = \pi_\theta(u_1 \mid x_1) \\
p(x_2 \mid x_0, u_0, x_1, u_1) = p(x_2 \mid x_1, u_1)$

따라서 최종적으로 아래와 같은 형태가 된다:

$p_\theta(\tau) = p(x_0) \prod_{t=0}^{T} \pi_\theta(u_t \mid x_t)p(x_{t+1} \mid x_t, u_t)$

---

#### 2. 목적 함수 $J(\theta)$ 정의

목적 함수는 state value function과도 관계가 있으며, 다음과 같이 정의된다:

$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right]$

적분으로 표현하면:

$J(\theta) = \int_\tau p_\theta(\tau) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau$

Trajectory $\tau$를 다음과 같이 분할할 수 있다:

$\tau = (x_0, u_0, x_1, u_1, \dots, x_T, u_T) = (x_0) \cup \tau_{u_0:u_T}$

Chain rule에 의해:

$p_\theta(\tau) = p_\theta(x_0, \tau_{u_0:u_T}) = p_\theta(x_0) p_\theta(\tau_{u_0:u_T} \mid x_0)$  

이를 위 목적 함수에 대입하면:

$$\begin{align*}
J(\theta) &= \int_{x_0} \int_{\tau_{u_0:u_T}} p_\theta(x_0) p_\theta(\tau_{u_0:u_T} \mid x_0) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau_{u_0:u_T} dx_0 \\
&= \int_{x_0} \left[ \int_{\tau_{u_0:u_T}} p_\theta(\tau_{u_0:u_T} \mid x_0) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau_{u_0:u_T} \right] p(x_0) dx_0
\end{align*}$$  


대괄호 항은 상태가 $x_0$일 때의 **state value function**이므로 목적 함수는:

$J(\theta) = \int_{x_0} V^{\pi_\theta}(x_0) p(x_0) dx_0 = \mathbb{E}_{x_0 \sim p(x_0)}[V^{\pi_\theta}(x_0)]$  


---

#### 즉, 목적 함수 $J(\theta)$는 초기 상태 변수 $x_0$에 대한 상태 가치의 평균값이며, 강화학습에서 에이전트가 최대화하고자 하는 대상이다.

---

#### 3. 목적 함수의 그래디언트 계산

$\nabla_\theta J(\theta) = \nabla_\theta \int_\tau p_\theta(\tau) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right) d\tau$  


log-미분 성질 사용:


= $\int_\tau p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right) d\tau$

Trajectory 확률의 log-그래디언트는 다음과 같다:

$\nabla_\theta \log p_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t)$  


따라서 정책 그래디언트 정리는 다음과 같다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\left( \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \right) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right)\right]$  

![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%5Cleft%28%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cright%29%20%5Cleft%28%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cgamma%5Et%20r%28x_t%2C%20u_t%29%20%5Cright%29%5Cright%5D)  


---

#### 4. 인과성 반영 (Causality 고려)

시간 $t$에 선택된 행동은 그 이후 보상에만 영향을 준다는 인과성 원칙을 반영하면 다음과 같이 수정된다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \left( \sum_{k=t}^T \gamma^{k - t} r(x_k, u_k) \right)\right]$  


![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cleft%28%20%5Csum_%7Bk%3Dt%7D%5ET%20%5Cgamma%5E%7Bk%20-%20t%7D%20r%28x_k%2C%20u_k%29%20%5Cright%29%5Cright%5D)  



---

#### 5. 감가율 $\gamma$의 영향

감가율 $\gamma^t$는 log-policy gradient에 곱해지기 때문에 시간이 지남에 따라 기여도가 점점 작아진다.

- $\gamma \to 0$: 미래 보상 영향 감소 → 빠르게 수렴하지만 후반 정보 손실
- $\gamma = 1$: 전 보상에 평등한 중요도 → variance 커질 수 있음

---

#### 6. 실용적인 정책 그래디언트 형태

보다 안정적인 학습을 위해, 실용적인 정책 그래디언트는 다음과 같은 형태로 사용된다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \left( \sum_{k=t}^T \gamma^{k-t} r(x_k, u_k) \right) \right]$  

![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cleft%28%20%5Csum_%7Bk%3Dt%7D%5ET%20%5Cgamma%5E%7Bk-t%7D%20r%28x_k%2C%20u_k%29%20%5Cright%29%20%5Cright%5D)  



위의 형태는 **REINFORCE 알고리즘**의 핵심이며, 모델-프리 강화학습에서 널리 사용된다.

강화학습에서 함수 근사와 딥 강화학습 부분을 수식적 내용을 포함하여 더 상세하게 확장하겠습니다.

# 10. 함수 근사와 딥 강화학습

## 10.1 함수 근사(Function Approximation)의 필요성과 수학적 기반

현실 세계의 복잡한 강화학습 문제에서는 상태 공간이 매우 크거나 연속적이기 때문에 테이블 형태(tabular)로 모든 상태-행동 쌍의 가치를 저장하는 것이 불가능합니다. 이러한 한계를 극복하기 위해 함수 근사 방법을 사용합니다.

### 10.1.1 함수 근사의 수학적 정의

가치 함수 $V(s)$ 또는 $Q(s,a)$를 파라미터 $\theta$를 가진 함수 근사기로 표현:

$$V(s) \approx \hat{V}(s,\theta)$$
$$Q(s,a) \approx \hat{Q}(s,a,\theta)$$

여기서 $\hat{V}$와 $\hat{Q}$는 파라미터 $\theta$에 의해 결정되는 근사 함수입니다.

### 10.1.2 손실 함수와 최적화

함수 근사에서는 일반적으로 다음과 같은 손실 함수를 최소화합니다:

**평균 제곱 오차(MSE):**
$$L(\theta) = \mathbb{E}_{s \sim \mu(s)}\left[(V^\pi(s) - \hat{V}(s,\theta))^2\right]$$

여기서 $\mu(s)$는 상태 분포이며, $V^\pi(s)$는 실제 가치 함수입니다.

**경사 하강법(Gradient Descent):**
$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta L(\theta_t)$$

$$\nabla_\theta L(\theta_t) = -\mathbb{E}_{s \sim \mu(s)}\left[(V^\pi(s) - \hat{V}(s,\theta_t)) \nabla_\theta \hat{V}(s,\theta_t)\right]$$

실제 강화학습에서는 $V^\pi(s)$가 알려져 있지 않기 때문에, 대신 샘플된 반환값이나 부트스트랩 추정값을 사용합니다.

### 10.1.3 선형 함수 근사

가장 간단한 형태의 함수 근사는 선형 함수 근사입니다:

$$\hat{V}(s,\theta) = \theta^T \phi(s) = \sum_{i=1}^{d} \theta_i \phi_i(s)$$

여기서 $\phi(s)$는 상태 $s$의 특징 벡터(feature vector)이고, $\theta$는 가중치 벡터입니다.

**업데이트 규칙:**
$$\theta_{t+1} = \theta_t + \alpha [R_{t+1} + \gamma \hat{V}(S_{t+1},\theta_t) - \hat{V}(S_t,\theta_t)] \nabla_\theta \hat{V}(S_t,\theta_t)$$

선형 함수 근사에서는:
$$\nabla_\theta \hat{V}(S_t,\theta_t) = \phi(S_t)$$

따라서 업데이트 규칙은:
$$\theta_{t+1} = \theta_t + \alpha [R_{t+1} + \gamma \hat{V}(S_{t+1},\theta_t) - \hat{V}(S_t,\theta_t)] \phi(S_t)$$

## 10.2 딥 강화학습(Deep Reinforcement Learning)의 이론적 기반

딥 강화학습은 가치 함수나 정책을 신경망으로 근사하는 방법입니다. 복잡한 패턴 인식 능력을 가진 딥러닝과 결합하여 강화학습의 성능을 크게 향상시켰습니다.

### 10.2.1 DQN (Deep Q-Network)의 수학적 기반

DQN은 Q-학습에 딥 뉴럴 네트워크를 적용한 알고리즘입니다.

**Q-함수 근사:**
$$Q(s,a) \approx Q(s,a;\theta)$$

여기서 $Q(s,a;\theta)$는 파라미터 $\theta$를 가진 신경망입니다.

**손실 함수:**
$$L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta))^2\right]$$

여기서 $\mathcal{D}$는 경험 리플레이 버퍼이고, $\theta^-$는 타겟 네트워크의 파라미터입니다.

**그래디언트:**
$$\nabla_\theta L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta)) \nabla_\theta Q(s,a;\theta)\right]$$

**DQN의 주요 혁신:**
1. **경험 리플레이(Experience Replay)**: 시간적 상관관계를 줄이고 학습 안정성 향상
2. **타겟 네트워크(Target Network)**: Q값 추정의 안정성 증가를 위해 고정된 목표 제공

### 10.2.2 정책 경사법(Policy Gradient)과 Actor-Critic 방법

정책 경사법은 정책 $\pi_\theta$를 직접 최적화합니다.

**목적 함수:**
$$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=0}^{T} r(s_t, a_t) \right]$$

**정책 경사(Policy Gradient):**
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum_{k=t}^{T} \gamma^{k-t} r(s_k, a_k) \right]$$

**Actor-Critic 방법:**
Actor-Critic은 정책(Actor)과 가치 함수(Critic)를 동시에 학습합니다.

$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}\left[ \nabla_\theta \log \pi_\theta(a|s) Q^\pi(s,a) \right]$$

크리틱은 상태-행동 가치 함수 $Q^\pi(s,a)$를 근사하고, 액터는 이 가치 평가를 기반으로 정책 $\pi_\theta$를 개선합니다.

**Advantage Actor-Critic(A2C):**
학습 안정성을 높이기 위해 Advantage 함수를 사용합니다:

$$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$$

업데이트 식:
$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}\left[ \nabla_\theta \log \pi_\theta(a|s) A^\pi(s,a) \right]$$

### 10.2.3 DDPG (Deep Deterministic Policy Gradient)

DDPG는 연속 행동 공간에서 작동하는 오프-폴리시 액터-크리틱 알고리즘입니다.

**결정론적 정책:**
$$\mu_\theta: \mathcal{S} \rightarrow \mathcal{A}$$

**크리틱 목적 함수:**
$$L(\phi) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[(r + \gamma Q(s', \mu_{\theta'}(s');\phi') - Q(s,a;\phi))^2\right]$$

**액터 목적 함수:**
$$J(\theta) = \mathbb{E}_{s \sim \mathcal{D}}\left[Q(s, \mu_\theta(s);\phi)\right]$$

**액터 그래디언트:**
$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \mathcal{D}}\left[\nabla_a Q(s,a;\phi)|_{a=\mu_\theta(s)} \cdot \nabla_\theta \mu_\theta(s)\right]$$

### 10.2.4 PPO (Proximal Policy Optimization)

PPO는 정책 업데이트 시 급격한 변화를 방지하기 위해 신뢰 영역 제약을 사용합니다.

**클리핑된 목적 함수:**
$$L^{CLIP}(\theta) = \mathbb{E}_t\left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]$$

여기서 $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$는 확률 비율이며, $\epsilon$은 클리핑 파라미터입니다.

### 10.2.5 SAC (Soft Actor-Critic)

SAC는 최대 엔트로피 강화학습 프레임워크를 사용하는 오프-폴리시 알고리즘입니다.

**목적 함수:**
$$J(\theta) = \mathbb{E}_{s_t \sim \mathcal{D}}\left[ \mathbb{E}_{a_t \sim \pi_\theta(\cdot|s_t)}\left[ Q(s_t, a_t) - \alpha \log \pi_\theta(a_t|s_t) \right] \right]$$

여기서 $\alpha$는 엔트로피 계수로, 탐험과 이용 사이의 균형을 조절합니다.

**정책 그래디언트:**
$$\nabla_\theta J(\theta) = \mathbb{E}_{s_t \sim \mathcal{D}}\left[ \nabla_\theta \mathbb{E}_{a_t \sim \pi_\theta(\cdot|s_t)}\left[ Q(s_t, a_t) - \alpha \log \pi_\theta(a_t|s_t) \right] \right]$$

재매개변수화 트릭(reparameterization trick)을 사용하여:
$$a_t = f_\theta(s_t, \xi), \quad \xi \sim \mathcal{N}(0, I)$$

그래디언트는 다음과 같이 계산됩니다:
$$\nabla_\theta J(\theta) = \mathbb{E}_{s_t \sim \mathcal{D}, \xi \sim \mathcal{N}}\left[ \nabla_\theta f_\theta(s_t, \xi) \nabla_a \left( Q(s_t, a) - \alpha \log \pi_\theta(a|s_t) \right)|_{a=f_\theta(s_t, \xi)} \right]$$

## 10.3 경험 리플레이(Experience Replay)의 이론적 분석

### 10.3.1 수학적 정의

경험 리플레이는 에이전트의 경험 $(s_t, a_t, r_t, s_{t+1})$을 메모리 버퍼 $\mathcal{D}$에 저장하고 학습에 재사용하는 기법입니다.

**중요성 샘플링(Importance Sampling):**
오프-폴리시 학습에서는 다른 정책으로부터 생성된 데이터를 사용하기 위해 중요성 샘플링을 적용할 수 있습니다:

$$\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[ \frac{p(x)}{q(x)} f(x) \right]$$

경험 리플레이에 적용하면:
$$\mathbb{E}_{(s,a) \sim \pi_\theta}\left[ Q^\pi(s,a) \right] = \mathbb{E}_{(s,a) \sim \mathcal{D}}\left[ \frac{\pi_\theta(a|s)}{\mu(a|s)} Q^\pi(s,a) \right]$$

여기서 $\mu$는 데이터를 생성한 행동 정책입니다.

### 10.3.2 Prioritized Experience Replay (PER)

모든 전이(transition)가 동일하게 중요하지 않다는 아이디어에서 출발한 PER은 TD 오차가 큰 전이에 더 높은 우선순위를 부여합니다.

**샘플링 확률:**
$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

여기서 $p_i$는 전이 $i$의 우선순위이고, $\alpha$는 우선순위 정도를 결정합니다.

**TD 오차 기반 우선순위:**
$$p_i = |\delta_i| + \epsilon$$

여기서 $\delta_i = r_i + \gamma \max_{a'} Q(s_{i+1}, a'; \theta) - Q(s_i, a_i; \theta)$는 TD 오차이고, $\epsilon$은 작은 상수입니다.

**중요성 샘플링 가중치:**
편향을 보정하기 위해 중요성 샘플링 가중치를 적용합니다:
$$w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta$$

여기서 $\beta$는 0에서 1 사이의 값으로, 편향 보정 정도를 조절합니다.

## 10.4 멀티 에이전트 강화학습(Multi-Agent RL)의 수학적 기반

### 10.4.1 게임 이론적 설정

$n$명의 에이전트가 있는 멀티 에이전트 환경은 게임 $G = (N, S, A, T, R, \gamma)$로 모델링할 수 있습니다:
- $N = \{1, 2, ..., n\}$: 에이전트 집합
- $S$: 상태 공간
- $A = A_1 \times A_2 \times ... \times A_n$: 결합 행동 공간
- $T: S \times A \times S \rightarrow [0, 1]$: 상태 전이 함수
- $R = (R_1, R_2, ..., R_n)$: 보상 함수 집합
- $\gamma$: 할인 인자

### 10.4.2 Nash Equilibrium

내쉬 균형은 다른 에이전트들의 정책이 고정되었을 때, 어떤 에이전트도 자신의 정책을 단독으로 변경하여 이득을 볼 수 없는 정책 조합입니다.

$$\pi^* = (\pi_1^*, \pi_2^*, ..., \pi_n^*)$$

이며, 모든 에이전트 $i$와 가능한 모든 정책 $\pi_i$에 대해:

$$V_i^{\pi_i^*, \pi_{-i}^*} \geq V_i^{\pi_i, \pi_{-i}^*}$$

여기서 $\pi_{-i}^*$는 에이전트 $i$를 제외한 모든 에이전트의 최적 정책 집합입니다.

### 10.4.3 MADDPG (Multi-Agent DDPG)

MADDPG는 각 에이전트가 다른 에이전트들의 행동을 고려하는 중앙 집중식 학습, 분산 실행 방식의 알고리즘입니다.

**중앙 집중식 비평(Centralized Critic):**
각 에이전트 $i$의 크리틱은 모든 에이전트의 관찰과 행동을 입력으로 받습니다:
$$Q_i^\mu(x, a_1, ..., a_n)$$

**분산 실행 행동자(Decentralized Actor):**
각 에이전트의 액터는 자신의 관찰만을 입력으로 받습니다:
$$\mu_i(o_i)$$

**정책 경사:**
$$\nabla_{\theta_i} J(\mu_i) = \mathbb{E}_{x,a \sim \mathcal{D}}\left[ \nabla_{\theta_i} \mu_i(o_i) \nabla_{a_i} Q_i^\mu(x, a_1, ..., a_n)|_{a_i=\mu_i(o_i)} \right]$$

## 10.5 계층적 강화학습(Hierarchical RL)의 수학적 프레임워크

### 10.5.1 옵션 프레임워크(Option Framework)

옵션은 시작 조건, 정책, 종료 조건으로 구성된 확장된 행동입니다:
$$o = (I_o, \pi_o, \beta_o)$$

- $I_o \subseteq S$: 옵션 $o$의 시작 상태 집합
- $\pi_o: S \times A \rightarrow [0, 1]$: 옵션 $o$의 정책
- $\beta_o: S \rightarrow [0, 1]$: 옵션 $o$의 종료 확률

### 10.5.2 옵션 가치 함수

옵션 $o$를 선택했을 때의 가치 함수는 다음과 같이 정의됩니다:

$$Q^\Omega(s, o) = \mathbb{E}\left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} | s_t = s, o_t = o \right]$$

### 10.5.3 Feudal Networks

Feudal 네트워크는 상위 관리자(manager)와 하위 작업자(worker)로 구성된 계층적 구조입니다.

**관리자 목표 설정:**
$$g_t = f_M(h_t^M)$$

여기서 $f_M$은 관리자 네트워크이고, $h_t^M$은 관리자의 은닉 상태입니다.

**작업자 정책:**
$$\pi_W(a_t | s_t, g_t)$$

**관리자 보상:**
$$r_t^M = \sum_{i=t}^{t+c-1} \nabla_{g_t} U_i^W$$

여기서 $U_i^W$는 작업자의 유틸리티이고, $c$는 시간 스케일 파라미터입니다.

## 10.6 메타 강화학습(Meta RL)의 수학적 접근

### 10.6.1 MAML (Model-Agnostic Meta-Learning)

MAML은 새로운 작업에 빠르게 적응할 수 있는 초기 파라미터를 학습하는 방법입니다.

**목적 함수:**
$$\min_\theta \sum_{T_i \sim p(T)} \mathcal{L}_{T_i}(f_{\theta'_i})$$

여기서 $\theta'_i = \theta - \alpha \nabla_\theta \mathcal{L}_{T_i}(f_\theta)$는 작업 $T_i$에 대한 한 단계 적응 후의 파라미터입니다.

### 10.6.2 RL² (RL-Squared)

RL²는 에피소드들 간의 정보를 활용하는 메타 강화학습 알고리즘입니다.

**확장된 상태 표현:**
$$\tilde{s}_t = (s_t, a_{t-1}, r_{t-1}, d_{t-1})$$

여기서 $d_{t-1}$은 이전 에피소드가 종료되었는지 여부를 나타내는 지표입니다.

**RNN 기반 정책:**
$$\pi_\theta(a_t | \tilde{s}_t, h_t)$$

여기서 $h_t$는 RNN의 은닉 상태로, 이전 경험을 요약합니다.

## 10.7 강화학습의 최신 연구 동향과 수학적 도전 과제

### 10.7.1 오프라인 강화학습(Offline RL)

오프라인 강화학습은 환경과의 상호작용 없이 사전 수집된 데이터만으로 학습하는 방법입니다.

**분포 제약 정규화(Distributional Constraint Regularization):**
$$\max_\pi \mathbb{E}_{s \sim d^\beta, a \sim \pi(\cdot|s)}\left[ Q^\pi(s,a) - \alpha D(\pi(\cdot|s) || \beta(\cdot|s)) \right]$$

여기서 $\beta$는 행동 정책이고, $D$는 두 분포 간의 거리 측정입니다.

### 10.7.2 인과 강화학습(Causal RL)

인과 관계를 고려한 강화학습은 행동과 결과 사이의 인과 관계를 모델링합니다.

**인과 효과:**
$$P(Y = y | do(X = x))$$

여기서 $do(X = x)$는 변수 $X$를 $x$로 설정하는 개입(intervention)을 나타냅니다.

### 10.7.3 불확실성을 고려한 강화학습

모델 불확실성과 행동 불확실성을 명시적으로 고려하는 접근법입니다.

**베이지안 신경망(Bayesian Neural Networks):**
$$p(y|x, D) = \int p(y|x, \theta) p(\theta|D) d\theta$$

여기서 $p(\theta|D)$는 데이터 $D$가 주어졌을 때 모델 파라미터 $\theta$의 사후 분포입니다.

**불확실성 분해:**
$$\text{Var}[y|x, D] = \underbrace{\mathbb{E}_{\theta \sim p(\theta|D)}[\text{Var}[y|x, \theta]]}_{\text{행동 불확실성(Aleatoric)}} + \underbrace{\text{Var}_{\theta \sim p(\theta|D)}[\mathbb{E}[y|x, \theta]]}_{\text{모델 불확실성(Epistemic)}}$$

## 10.8 함수 근사와 딥 강화학습의 수렴성 분석

함수 근사를 사용한 강화학습의 수렴성은 중요한 이론적 질문입니다.

### 10.8.1 선형 함수 근사에서의 수렴성

TD 학습의 경우, 다음 조건에서 수렴이 보장됩니다:
- 상태 분포가 정상(stationary)이고,
- 특징 벡터가 유한한 norm을 가지며,
- 학습률이 Robbins-Monro 조건을 만족할 때.

**TD(0) 수렴 조건:**
$$\sum_{t=1}^{\infty} \alpha_t = \infty, \quad \sum_{t=1}^{\infty} \alpha_t^2 < \infty$$

### 10.8.2 비선형 함수 근사와 수렴성 도전

비선형 함수 근사(예: 신경망)를 사용할 경우, 일반적인 수렴 보장이 어렵습니다. 그러나 특정 조건에서 근사적 수렴이 가능합니다:

**경험 리플레이와 타겟 네트워크의 역할:**
- 경험 리플레이는 샘플 간 상관관계를 줄여 수렴성 향상
- 타겟 네트워크는 목표값 안정화에 기여

이러한 이론적 분석은 딥 강화학습 알고리즘의 설계 및 개선에 중요한 지침을 제공합니다.

---

이로써 함수 근사와 딥 강화학습에 관한 수식적 내용을 포함한 확장된 설명을 제공했습니다. 이 내용은 기본 개념뿐만 아니라 수학적 기반, 알고리즘 분석, 그리고 최신 연구 동향까지 포괄하고 있습니다.
