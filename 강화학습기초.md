# 1. 확률
불확실성은 사건의 각각이 일어날 Likelihood 와 probability로 나타낼  수 있다.
발생할 수 있는 모든 상황을 world라고 생각할 수 있으며, w로 나타낸다. 예를 들면 주사위를 던진 결과는 6 world 가 될 수 있으며, 
특정 세계의 확률을 P(w)로 나타낸다.

### 1.1 확률의 공리
확률을 나타내는 값은 0부터 1까지의 범위에 들어야한다.  $$0 <= P(w) <=1$$  

0은 불가능한 사건을 의미하며, 1은 무조건 일어나는 사건  

 $$\Sigma  P(w) = 1$$

### 1.2 조건부 확률
이미 밝혀진 일부 증거가 주어졌을 때, 명제에 대한 믿음의 정도이다
사건이 미래에 발생할 확률에 영향을 미치는 정보를 사용하기 위해 조건부 확률에 의존한다.  

$$P(A|B) =  \frac {P(A \cap B)}{P(B)}$$

### 1.3 베이즈 정리 (Bayes Rule)

사건 B가 발생한다는 가정하에 사건 A가 발생할 확률을 사건 A의 조건부 확률이라고 하고 아래와 같이 정리할 수 있다.  

$$P(A|B) =  \frac {P(A , B)}{P(B)}$$

이와 비슷하게 사건 A가 발생한다는 가정하에 사건 B가 발생할 확률은 아래와 같다.  

$$P(B|A) = \frac {P(A , B)}{P(A)}$$

위의 두 식을 이용하여 chain rule을 만들면
$$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$$
우측 그림과 같이 N개의 사건이 서로 배타적이고 $\Sigma_{i=1} ^n P(B_i) = 1$ 이면 , 
임의의 사건 A의 확률은 아래와 같다.  

$$P(A) = \Sigma_{i=1} ^n P(A, B_i) = \Sigma_{i=1} ^n P(A|B_i)P(B_i)$$
위의 식을 total probability 라고 하며, 사건 A를 조건으로 하는 임의의 사건 $ B_i $ 의 조건부 확률을 Chain Rule을 이용해 표현하면  

$$P(B_i|A) = \frac {P(A,B_i)} {P(A)} = \frac {P(A|B_i)P(B_i)} {P(A)}$$  

위의 식을 total probability 정리를 대입하면  

$$P(B_i|A) = \frac {P(A|B_i)P(B_i)} {\Sigma_{i=1} ^nP(A|B_i)P(B_i)}$$ 

# 2. Markov model 
 여러 개의 상태가 존재하고 상태 간의 전지 확률을 Markov property로 정의  
 Markov Property는 t+1 에서의 상태는 오직 t 에서의 상태에 의해서만 영향을 받는다.

#### 상황에 따라 4 가지 유형

| 모델 | 제어 가능성 | 상태 관찰 가능성 | 예시 |
|------|-------------|------------------|------|
| **Markov Chain** | ✖ (제어 없음) | ✔ 완전 관찰 | 날씨 예측 |
| **Hidden Markov Model (HMM)** | ✖ (제어 없음) | ✖ 부분 관찰 | 음성 인식 |
| **Markov Decision Process (MDP)** | ✔ (에이전트가 행동함) | ✔ 완전 관찰 | 게임, 로봇 제어 |
| **Partially Observable MDP (POMDP)** | ✔ (에이전트가 행동함) | ✖ 부분 관찰 | 센서 기반 로봇 제어 |
  
# 3. 강화학습 알고리즘 계통도 정리

### 3.1 MDP (Markov Decision Process)
- 강화학습의 이론적 토대
- 상태, 행동, 보상, 상태 전이로 구성된 모델

### 3.2 DP (Dynamic Programming)
- 결정론적 환경에서 효과적이며 직관적이고 MDP에 비해 적은 계산량량
- 모델(전이 확률)을 알고 있어야 사용 가능
- 현실에서는 사용이 제한적 → 경험 기반 학습 필요

### 3.3 몬테카를로 방식 (Monte Carlo Method)
- 모델 없이 경험 데이터를 통해 학습
- 에피소드가 끝난 후에만 업데이트 가능 → 느림

### 3.4 TD 학습 (Temporal Difference)
- 몬테카를로 + DP의 장점 결합
- 에피소드 도중에도 업데이트 가능

---

## 🔷 TD 기반 분류

### 📌 A. **가치 기반 강화학습 (Value-Based Reinforcement Learning)**
에이전트가 환경과 상호작용하며 **"행동의 가치를 학습"**하는 방식  
핵심은 "어떤 행동이 좋은가?"를 수치로 평가해서, 최적의 행동을 선택하는 것  
- 가치 함수(Value Function)를 학습하여 행동 결정
- **주요 알고리즘**:
  - SARSA → Deep SARSA
  - Q-Learning → DQN ( Neural ODE와 연결가능성 )
- **특징**:
  - 정책을 명시적으로 표현하지 않음
  - 가치 함수 기반 행동 선택

### 📌 B. **정책 기반 강화학습 (Policy-Based Reinforcement Learning)**
정책 기반 강화학습은 에이전트가 직접 정책(Policy)을 학습하여  
상태에서 어떤 행동을 할지를 확률적으로 결정하는 방식
- 직접 정책(π)을 최적화
- **주요 알고리즘**:
  - REINFORCE
  - A2C (Advantage Actor-Critic)
  - Continuous A2C
  - A3C (Asynchronous Advantage Actor-Critic)
  - PPO (Proximal Policy Optimization)
  - DDPG, TD3, SAC ...
- **특징**:
  - 연속 행동 공간에 유리
  - 안정적인 수렴 성질
  - 가치 함수와 정책을 함께 사용하는 Actor-Critic 구조
# 4. 강화학습 : 탐험 (Exploration), 이용 (Exploitation),  ε-Greedy

### 4.1 **탐험 (Exploration)**

- **정의**: 에이전트가 **새로운 행동을 시도**하여 **모르는 정보를 얻는 과정**입니다. 에이전트는 **무작위 행동을 선택**하며, 이를 통해 다양한 경험을 쌓습니다.
- **목표**: 현재 상태에서 **최적의 행동**을 알지 못하므로 다양한 행동을 시도하여 더 나은 경로를 찾기 위함입니다.
- **장점**: 
  - **새로운 가능성**을 발견할 수 있음.
  - **전체적인 보상 최적화**를 위한 정보 획득.
- **단점**: 
  - **즉각적인 보상**을 얻기 어려움.
  - 최적의 경로를 놓칠 가능성 있음.

### 4.2 **이용 (Exploitation)**

- **정의**: 에이전트가 **이미 알고 있는 정보를 바탕으로 가장 좋은 행동을 선택**하는 과정입니다. 즉, **가장 높은 Q-value**를 가진 행동을 취함.
- **목표**: **최적의 행동**을 통해 **즉각적인 보상**을 최대화하려는 목적.
- **장점**: 
  - **효율적으로 목표**를 달성할 수 있음.
  - **빠른 성과**를 낼 수 있음.
- **단점**: 
  - **새로운 정보**를 얻지 못함.
  - **최적의 경로**만 따르게 되어 **다양한 가능성**을 놓칠 수 있음.

### 4.3 **입실론-그리디 (ε-Greedy)**

- **정의**: **탐험**과 **이용**을 **균형** 있게 수행하는 방법입니다. **ε(입실론)** 값을 사용하여 에이전트가 **탐험과 이용 사이에서 선택**하도록 합니다.
  - **ε (epsilon)**: 탐험과 이용의 비율을 결정하는 값으로, **0과 1** 사이의 값입니다.
  - **그리디(Exploitation)**: **ε의 확률**로 **현재까지 가장 좋은 행동**을 선택합니다.
  - **탐험(Exploration)**: **1-ε**의 확률로 **무작위 행동**을 선택하여 새로운 경험을 얻습니다.

### **ε-Greedy 동작 방식:**
- **ε 값이 작을수록**: 에이전트는 **주로 이용(Exploitation)**을 하며, **탐험(Exploration)**은 적게 시도
- **ε 값이 클수록**: 에이전트는 **탐험(Exploration)**을 많이 하며, **이용(Exploitation)**은 적게 시도

### **예시**:
- **ε = 0.1**: 90% 확률로 가장 높은 Q-value를 가진 행동을 선택하고, 10% 확률로 무작위로 행동을 선택.  
 즉, ε 값은 무작위 선택 확률을 의미
## 4.4 **탐험 vs 이용**

| 구분         | **탐험 (Exploration)**                  | **이용 (Exploitation)**                |
|--------------|--------------------------------------|--------------------------------------|
| **목표**     | 새로운 정보 얻기                        | 최적의 행동 선택                    |
| **행동 방식** | 무작위 행동 선택 (새로운 경험 쌓기)       | 이미 알고 있는 최적 행동 선택        |
| **장점**     | 새로운 경로와 가능성 발견               | 즉각적인 보상 최적화                |
| **단점**     | 최적 경로를 놓칠 수 있음                 | 새로운 정보 습득 어려움             |

## 4.5 **탐험과 이용의 균형**

강화학습에서 중요한 점은 **탐험(Exploration)**과 **이용(Exploitation)**의 균형입니다. 항상 이용만 하게 되면 최적의 경로를 반복하게 되어 새로운 가능성을 놓칠 수 있고, 반대로 항상 탐험만 하게 되면 효율적으로 목표를 달성하기 어려워집니다.

**입실론-그리디 (ε-Greedy)** 방법은 이 두 가지를 **적절히 조절**하는 방식으로, 에이전트가 **점진적으로 최적의 정책**을 학습할 수 있도록 도와줍니다.

# 5. Markov Decision Process (MDP)

이 문서는 강화학습의 핵심이 되는 **Markov Decision Process (MDP)**의 구조와 개념을 정리한 자료입니다.  
특히 **정책(Policy)**과 **상태 전이 확률(Transition Probability)**이 갖는 **Markov 성질**을 시각적으로 설명합니다.

---

## 1️⃣ MDP 구조

### 📌 상태와 행동의 흐름

- 상태(State): \( $s_0$ , $s_1$, $s_2$ \)
- 행동(Action): \( $a_0$ , $a_1$, $a_2$ \)
- 상태 전이(State Transition)는 행동을 통해 이루어짐

✅ 이 흐름은 에이전트가 **상태에서 행동을 선택하고**, 그 결과로 **다음 상태로 전이**되는 MDP의 기본 구조를 보여줍니다.

---

## 2️⃣ Markov Property의 의미

강화학습은 **Markov 가정**을 기반으로 동작합니다.

> "미래는 오직 현재에만 의존한다.  
> 과거는 더 이상 영향을 주지 않는다."

---

## 3️⃣ Policy의 Markov 성질

### 🎯 수식

\[
P($a_1$ | $s_0$, $a_0$, $s_1$) = P($a_1$ | $s_1$ )
\]

### 🔍 해석

- 정책(Policy)은 과거 상태나 행동이 아닌, **현재 상태 \(s_1\)** 만을 기반으로 **다음 행동 \(a_1\)** 을 선택합니다.
- 과거 정보 \(s_0, a_0\)는 정책 결정에 영향을 미치지 않습니다.

✅ 즉, 정책은 **현재 상태에서의 행동 선택 확률**만 정의합니다.

---

## 4️⃣ Transition Probability의 Markov 성질

### 🎯 수식

\[
$(s_2 | s_0, a_0, s_1, a_1) = P(s_2 | s_1, a_1)$
\]

### 🔍 해석

- 다음 상태 \(s_2\)는 오직 **현재 상태 \(s_1\)**와 **현재 행동 \(a_1\)**에만 의존합니다.
- 과거 상태와 행동은 전이 확률에 영향을 주지 않습니다.

✅ 상태 전이 확률도 **Markov 성질**을 따릅니다.

---

## 5️⃣ 강화학습의 목표 🎯

> 강화학습의 핵심 목표는 **Expected Return (기댓 보상)** 을 최대화하는 정책을 찾는 것입니다.

### 🔢 Return 수식

$$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots$$

- ($R_t$): 시점 $t$에서의 보상
- ($\gamma$): 할인율 (0 ≤ γ ≤ 1), 미래 보상의 현재 가치 반영

---

## ✅ 요약 정리

| 항목                  | 설명                                                                 |
|---------------------|----------------------------------------------------------------------|
| *State*           | 환경의 현재 상황 \($s_t$\)                                             |
| *Action*          | 에이전트가 상태에서 취하는 행동 ($a_t$\)                             |
| *Policy*          | 상태에 따라 행동을 선택하는 전략                       |
| *Transition Probability* | 행동 후 다음 상태로 전이될 확률                 |
| *Markov Property* | 다음 상태는 현재 상태와 행동에만 의존                              |
| *Return \($G_t$)*  | 누적 보상의 합, 미래 보상에 할인율 적용                             |
| **목표**            | Return을 최대화하는 최적 정책 \($\pi^*$\) 학습                         |

---

> 💡 핵심 정리:  
MDP에서는 에이전트가 현재 상태만을 고려하여 행동을 선택하며, 전이 또한 현재 정보만으로 결정됩니다.  
강화학습의 목표는 이 구조를 활용하여 **미래 보상의 합(Return)을 최대화**하는 정책을 찾는 것입니다.  
# 6-1. Ballman Equation 
## about State value function & Action value function

## ✅ 개념 정리

### 🔹 State Value Function (상태 가치 함수)

- **정의**:  
  현재 상태에서 앞으로 기대되는 누적 보상 (Return)의 기대값
- **의미**:  
  지금 상태가 얼마나 "좋은지"를 평가하는 함수  
- **수식**:
  \[
  $V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]$
  \]

- **직관**:  
  큰 값을 가질수록 좋은 상태다 (좋은 정책이면 좋을 상태로 이동해야 함)

---

### 🔹 Action Value Function (행동 가치 함수)

- **정의**:  
  특정 상태에서 특정 행동을 했을 때 앞으로 기대되는 누적 보상의 기대값
- **의미**:  
  이 행동을 지금 이 상태에서 선택했을 때 얼마나 좋은가?
- **수식**:
  \[
 $Q^\pi(s, a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ]$
  \]

---

## 🎯 Return 정의

\[
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$
\]

- \( $\gamma$ \): discount factor (0 < γ ≤ 1)  
- 미래 보상의 현재 가치 반영

---

## 📐 수식적 정의

1. 상태 가치 함수 ( State value function ):

\[
$ V(s_t) = \int_{a_t}^\infty G_t(a_t, s_{t+1}, a_{t+1}, \dots \mid s_t) \, da_t $
\]

2. 행동 가치 함수 ( Action value function ):

\[
$Q(s_t, a_t) = \int_{s_{t+1}}^\infty G_t(s_{t+1}, a_{t+1}, s_{t+2}, a_{t+2}, \dots \mid s_t, a_t) \, ds_{t+1}$
\]

> 각 수식은 기대값 관점에서 **미래의 전체 trajectory**를 고려하는 연속 확률 표현입니다.

---

## 🏁 Optimal Policy란?

- 최적 정책은 **상태 가치 함수**를 **최대한으로 만드는 정책**입니다.
- 수식적으로는:

\[
$\pi^* = \arg\max_\pi V^\pi(s)$
\]

또는 아래처럼 **상태-행동 전이 확률 시퀀스를 통해 최대화하는** 정책이 최적 정책이 됩니다:

\[
$\pi^* \propto \max \left( 
P(a_t, s_t),\ 
P(a_{t+1}, s_{t+1}),\ 
\dots,\ 
P(a_\infty, s_\infty)
\right)$
\]

---

## 🧠 요약

| 항목 | 의미 | 수식 | 설명 |
|------|------|------|------|
| \( $V(s)$ \) | 상태 가치 | \( $\mathbb{E}[G_t \mid s] $\) | 상태만 보고 가치 판단 |
| \( $Q(s, a)$ \) | 행동 가치 | \( $\mathbb{E}[G_t \mid s, a] $\) | 상태 + 행동 조합으로 가치 판단 |
| \($\pi^*$\) | 최적 정책 | \( $\arg\max_\pi V^\pi(s) $\) | 가장 좋은 상태를 만드는 정책 |  
  
    

# 6-1 + Specification: Bellman Equation Detailed Analysis

## Return 정의

\[
$G_t \triangleq R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots$
\]

### 1. 상태 가치 함수 \(V(S_t)\)

상태 가치 함수 $V(S_t)$는 현재 상태 $S_t$에서 시작하는 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t P(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

여기서 $P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t)$는 상태 전이 확률이며, $P(a_t \mid S_t)$는 주어진 상태에서의 정책에 의해 선택된 행동의 확률입니다.

## 과정 분석

### 상태 전이 및 반환값 계산

\[
$\int_{a_t, S_{t+1}} G_t P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 $Q(S_t, a_t)$를 사용하여 다음과 같이 재표현할 수 있습니다:

\[
$\int_{a_t} Q(S_t, a_t) P(a_t \mid S_t) da_t$
\]

### 행동 가치 함수 계산

\[
$\int_{a_t, S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_t, S_{t+1} \mid S_t) da_t, S_{t+1}$
\]

이 과정을 통해 $V(S_t)$에서 $V(S_{t+1})$로의 전이가 이루어지며, 상태 간의 전이 확률 $P(S_{t+1} \mid S_t, a_t)$와 정책 $P(a_t \mid S_t)$을 반영합니다.

## 행동 가치 함수 \($Q(S_t, a_t)$\) 계산

행동 가치 함수 $Q(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 기대 반환값을 계산합니다.

\[
$Q(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

### 계산 과정

1. **첫 번째 단계의 계산:**

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_{t+1}, \ldots \mid S_{t+1}) dS_{t+1} : a_\infty \Rightarrow V(S_{t+1})$
\]

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(S_{t+1} \mid S_t, a_t) dS_{t+1}$
\]

2. **두 번째 단계의 계산:**

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma G_{t+1}) P(S_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) dS_{t+2} : a_\infty \Rightarrow Q(S_{t+1}, a_{t+1})$
\]

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma Q(S_{t+1}, a_{t+1})) P(S_{t+1}, a_{t+1} \mid S_t, a_t) da_{t+1}, S_{t+1}$
\]

# Optimal Policy Analysis

## 상태 가치 함수 \( $V(S_t)$ \)

상태 가치 함수 $V(S_t)$는 현재 상태에서 시작하여 취할 수 있는 모든 행동의 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

이는 최적의 행동 가치 함수 $Q^*(S_t, a_t)$를 사용하여 다음과 같이 표현됩니다:

\[
$V(S_t) = \int_{a_t} Q^*(S_t, a_t) P(a_t \mid S_t) da_t$
\]

여기서 $argmax P(a_t \mid S_t)$는 주어진 상태에서 최적의 행동을 선택합니다.

## 최적 행동 가치 함수 \( $Q^*(S_t, a_t)$ \)

최적 행동 가치 함수 $Q^*(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 최대 기대 반환값을 계산합니다.

\[
$Q^*(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 모든 가능한 이후 상태의 시퀀스를 고려하여 계산됩니다:

\[
$Q^*(S_t, a_t) = P(a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_{t}, a_t) P(S_{t+1} \mid S_t, a_t)$
\]

\[
= $P(S_{t+2}, a_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) P^*(a_{t+1} \mid S_{t+1})$
\]

\[
$...$
\]

## 7. Q - value 는 어떻게 찾을까? ( 강화학습에서의 Q 값 추정 )

# Monte-Carlo (MC) vs Temporal Difference (TD)

강화학습에서 $Q$-value를 추정하는 대표적인 두 가지 방법인 **Monte-Carlo(MC)** 방식과 **Temporal Difference(TD)** 방식은 서로 다른 철학을 갖고 접근근

---

## 🔹 Monte-Carlo 방법 (MC)

- **정의**: 전체 에피소드가 끝난 후, 실제 관측된 누적 보상 $G_t$를 평균하여 $Q$ 값을 추정
- **수식**:
  $$Q(s_t, a_t) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$$
- **특징**:
  - 전체 trajectory(궤적)를 따라가며 실제 리턴 $G_t$를 계산
  - 모델 없이 순수한 경험 기반 학습
- **장점**:
  - 불편추정(unbiased estimate)
  - 간단하고 직관적
- **단점**:
  - 에피소드가 끝날 때까지 기다려야 함 (느림)
  - 온라인 학습에 부적합

---

## 🔹 Temporal Difference 방법 (TD)
모델이 없거나(model-free), 모델 역학(model dynamics)을 미리 알 필요가 없는 비 에피소드 작업에도 적용 가능한 학습 알고리즘
- **정의**: 다음 상태의 $Q$ 값을 기반으로 한 단계씩 근사 추정
- **수식**:  

  $$Q(s_t, a_t) \approx R_t + \gamma Q(s_{t+1}, a_{t+1})$$
  또는 평균을 사용한 업데이트 방식:  

  $$Q_N = \frac{1}{N} \sum_{i=1}^{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) \right)$$
  그리고 점진적 업데이트:  
  
  $$Q_N = Q_{N-1} + \frac{1}{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) - Q_{N-1} \right)$$
- **특징**:
  - 리턴 전체를 기다리지 않고, 예측된 다음 $Q$ 값을 사용하여 즉시 업데이트
  - 학습 속도가 빠르고 실시간 학습에 적합
- **장점**:
  - 에피소드 종료를 기다릴 필요 없음 (빠름)
  - 온라인 학습 가능
- **단점**:
  - 추정값을 바탕으로 업데이트되므로 bias가 존재할 수 있음
  -  Monte Carlo 방법은 에피소드 환경에만 적용된다는 단점과 에피소드가 매우 길면 가치 함수를 계산하기 위해 오랜 시간이 걸림

---

## 📊 비교 정리

| 항목 | Monte-Carlo (MC) | Temporal Difference (TD) |
|------|------------------|---------------------------|
| 학습 시점 | 에피소드 종료 후 | 한 스텝마다 (온라인) |
| 보상 사용 | 실제 누적 보상 $G_t$ | 예상된 $Q(s_{t+1}, a_{t+1})$ |
| 추정 성질 | 불편추정 (Unbiased) | 편향추정 (Biased) |
| 데이터 기반 | 완전한 경험 | 예측 포함 |
| 장점 | 안정적, 직관적 | 빠름, 실시간 학습 가능 |
| 단점 | 느림, 오프라인 | 초기 불안정, 편향 가능성 |

---

## ✅ 결론

- Monte-Carlo는 **정확하지만 느리며**, 에피소드 단위로 학습합니다.
- Temporal Difference는 **빠르지만 추정 기반**으로 더 빠른 반응과 실시간 처리가 가능합니다.
- 실제 강화학습에서는 이 두 방식을 적절히 혼합한 **SARSA, Q-learning** 같은 알고리즘들이 널리 사용됩니다.

# 8. 강화학습 방법
