# 1. 확률
불확실성은 사건의 각각이 일어날 Likelihood 와 probability로 나타낼  수 있다.
발생할 수 있는 모든 상황을 world라고 생각할 수 있으며, w로 나타낸다. 예를 들면 주사위를 던진 결과는 6 world 가 될 수 있으며, 
특정 세계의 확률을 P(w)로 나타낸다.

### 1.1 확률의 공리
확률을 나타내는 값은 0부터 1까지의 범위에 들어야한다.  $$0 <= P(w) <=1$$  

0은 불가능한 사건을 의미하며, 1은 무조건 일어나는 사건  

 $$\Sigma  P(w) = 1$$

### 1.2 조건부 확률
이미 밝혀진 일부 증거가 주어졌을 때, 명제에 대한 믿음의 정도이다
사건이 미래에 발생할 확률에 영향을 미치는 정보를 사용하기 위해 조건부 확률에 의존한다.  

$$P(A|B) =  \frac {P(A \cap B)}{P(B)}$$

### 1.3 베이즈 정리 (Bayes Rule)

사건 B가 발생한다는 가정하에 사건 A가 발생할 확률을 사건 A의 조건부 확률이라고 하고 아래와 같이 정리할 수 있다.  

$$P(A|B) =  \frac {P(A , B)}{P(B)}$$

이와 비슷하게 사건 A가 발생한다는 가정하에 사건 B가 발생할 확률은 아래와 같다.  

$$P(B|A) = \frac {P(A , B)}{P(A)}$$

위의 두 식을 이용하여 chain rule을 만들면
$$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$$
우측 그림과 같이 N개의 사건이 서로 배타적이고 $\Sigma_{i=1} ^n P(B_i) = 1$ 이면 , 
임의의 사건 A의 확률은 아래와 같다.  

$$P(A) = \Sigma_{i=1} ^n P(A, B_i) = \Sigma_{i=1} ^n P(A|B_i)P(B_i)$$
위의 식을 total probability 라고 하며, 사건 A를 조건으로 하는 임의의 사건 $ B_i $ 의 조건부 확률을 Chain Rule을 이용해 표현하면  

$$P(B_i|A) = \frac {P(A,B_i)} {P(A)} = \frac {P(A|B_i)P(B_i)} {P(A)}$$  

위의 식을 total probability 정리를 대입하면  

$$P(B_i|A) = \frac {P(A|B_i)P(B_i)} {\Sigma_{i=1} ^nP(A|B_i)P(B_i)}$$ 

# 2. Markov model 
 여러 개의 상태가 존재하고 상태 간의 전지 확률을 Markov property로 정의  
 Markov Property는 t+1 에서의 상태는 오직 t 에서의 상태에 의해서만 영향을 받는다.

#### 상황에 따라 4 가지 유형

| 모델 | 제어 가능성 | 상태 관찰 가능성 | 예시 |
|------|-------------|------------------|------|
| **Markov Chain** | ✖ (제어 없음) | ✔ 완전 관찰 | 날씨 예측 |
| **Hidden Markov Model (HMM)** | ✖ (제어 없음) | ✖ 부분 관찰 | 음성 인식 |
| **Markov Decision Process (MDP)** | ✔ (에이전트가 행동함) | ✔ 완전 관찰 | 게임, 로봇 제어 |
| **Partially Observable MDP (POMDP)** | ✔ (에이전트가 행동함) | ✖ 부분 관찰 | 센서 기반 로봇 제어 |
  
# 3. 강화학습 알고리즘 계통도 정리

### 3.1 MDP (Markov Decision Process)
- 강화학습의 이론적 토대로 에이전트가 어떤 환경에서 보상을 최대화하기 위해 행동을 선택해야 하는 문제 자체를 수학적으로 모델링한 것
- <span style="color: green;"> 상태, 행동, 보상, 전이 확률</span> 4가지 요소로 구성된 모델

### 3.2 DP (Dynamic Programming)
- 결정론적 환경에서 효과적이며 직관적이고 MDP에 비해 적은 계산량
- 모델(전이 확률)을 알고 있어야 사용 가능 (DP 예시)
- 현실에서는 사용이 제한적 → 경험 기반 학습 필요

### 3.3 몬테카를로 방식 (Monte Carlo Method)
- 모델 없이 무작위 시뮬레이션을 여러번 수행해서 평균값이나 확률을 추정하는 방법
- *에피소드가 끝난 후에만 업데이트 가능 → 느림

### 3.4 TD 학습 (Temporal Difference)
- 몬테카를로 + DP의 장점 결합
- 매 타임스텝마다 가치를 업데이트 하는 방식으로 에피소드 과정에서 업데이트  
$V(s_t) \leftarrow V(s_t) + \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]$

### 3.5 부트스트래핑 (Bootstrapping)
- 현재의 추정값을 업데이트하기 위해 다른 추정값을 사용하는 방법
- TD 학습의 핵심 메커니즘으로, "추정값으로 추정값을 업데이트"하는 과정
- 수식: $V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]$에서 $V(s_{t+1})$이 부트스트래핑 부분
- 장점: 
  - 에피소드가 끝나기 전에 학습 가능 (온라인 학습)
  - 학습 속도가 빠름
  - 분산(variance)이 낮음
- 단점: 
  - 초기 추정값이 부정확하면 편향(bias)이 발생
  - 잘못된 추정값이 서로를 강화할 수 있음

**몬테카를로 vs TD(부트스트래핑) 비교**
- 몬테카를로: 실제 리턴 $G_t$ 사용 (불편향 추정) → 분산 높음, 속도 느림
- TD: 추정된 가치 $V(s_{t+1})$ 사용 (편향 추정) → 분산 낮음, 속도 빠름


## 🔷 TD 기반 분류

### 📌 A. **가치 기반 강화학습 (Value-Based Reinforcement Learning)**
에이전트가 환경과 상호작용하며 **"행동의 가치를 학습"**하는 방식  
핵심은 "어떤 행동이 좋은가?"를 수치로 평가해서, 최적의 행동을 선택하는 것  
- 가치 함수(Value Function)를 학습하여 행동 결정
- **주요 알고리즘**:
  - SARSA → Deep SARSA
  - Q-Learning → DQN ( Neural ODE와 연결가능성 )
- **특징**:
  - 정책을 명시적으로 표현하지 않음
  - 가치 함수 기반 행동 선택

### 📌 B. **정책 기반 강화학습 (Policy-Based Reinforcement Learning)**
정책 기반 강화학습은 에이전트가 직접 정책(Policy)을 학습하여  
상태에서 어떤 행동을 할지를 확률적으로 결정하는 방식
- 직접 정책(π)을 최적화
- **주요 알고리즘**:
  - REINFORCE
  - A2C (Advantage Actor-Critic)
  - Continuous A2C
  - A3C (Asynchronous Advantage Actor-Critic)
  - PPO (Proximal Policy Optimization)
  - DDPG, TD3, SAC ...
- **특징**:
  - 연속 행동 공간에 유리
  - 안정적인 수렴 성질
  - 가치 함수와 정책을 함께 사용하는 Actor-Critic 구조

# 4. 강화학습 : 탐험 (Exploration), 이용 (Exploitation),  ε-Greedy

### 4.1 **탐험 (Exploration)**

- **정의**: 에이전트가 **새로운 행동을 시도**하여 **모르는 정보를 얻는 과정**입니다. 에이전트는 **무작위 행동을 선택**하며, 이를 통해 다양한 경험을 쌓습니다.
- **목표**: 현재 상태에서 **최적의 행동**을 알지 못하므로 다양한 행동을 시도하여 더 나은 경로를 찾기 위함입니다.
- **장점**: 
  - **새로운 가능성**을 발견할 수 있음.
  - **전체적인 보상 최적화**를 위한 정보 획득.
- **단점**: 
  - **즉각적인 보상**을 얻기 어려움.
  - 최적의 경로를 놓칠 가능성 있음.

### 4.2 **이용 (Exploitation)**

- **정의**: 에이전트가 **이미 알고 있는 정보를 바탕으로 가장 좋은 행동을 선택**하는 과정입니다. 즉, **가장 높은 Q-value**를 가진 행동을 취함.
- **목표**: **최적의 행동**을 통해 **즉각적인 보상**을 최대화하려는 목적.
- **장점**: 
  - **효율적으로 목표**를 달성할 수 있음.
  - **빠른 성과**를 낼 수 있음.
- **단점**: 
  - **새로운 정보**를 얻지 못함.
  - **최적의 경로**만 따르게 되어 **다양한 가능성**을 놓칠 수 있음.

### 4.3 **입실론-그리디 (ε-Greedy)**

- **정의**: **탐험**과 **이용**을 **균형** 있게 수행하는 방법입니다. **ε(입실론)** 값을 사용하여 에이전트가 **탐험과 이용 사이에서 선택**하도록 합니다.
  - **ε (epsilon)**: 탐험과 이용의 비율을 결정하는 값으로, **0과 1** 사이의 값입니다.
  - **그리디(Exploitation)**: **(1-ε)의 확률**로 **현재까지 가장 좋은 행동**을 선택합니다.
  - **탐험(Exploration)**: **ε의 확률**로 **무작위 행동**을 선택하여 새로운 경험을 얻습니다.

### **ε-Greedy 동작 방식:**
- **ε 값이 작을수록**: 에이전트는 **주로 이용(Exploitation)**을 하며, **탐험(Exploration)**은 적게 시도
- **ε 값이 클수록**: 에이전트는 **탐험(Exploration)**을 많이 하며, **이용(Exploitation)**은 적게 시도
- 학습이 진행됨에 따라 점진적으로 ε 값을 줄여서 초기에는 탐험을 많이 하고 나중에는 이용을 많이 하도록 조절하는 방법도 많이 사용됨

### **예시**:
- **ε = 0.1**: 90% 확률로 가장 높은 Q-value를 가진 행동을 선택하고, 10% 확률로 무작위로 행동을 선택.  
 즉, ε 값은 무작위 행동(탐험)을 선택할 확률을 의미
## 4.4 **탐험 vs 이용**

| 구분         | **탐험 (Exploration)**                  | **이용 (Exploitation)**                |
|--------------|--------------------------------------|--------------------------------------|
| **목표**     | 새로운 정보 얻기                        | 최적의 행동 선택                    |
| **행동 방식** | 무작위 행동 선택 (새로운 경험 쌓기)       | 이미 알고 있는 최적 행동 선택        |
| **장점**     | 새로운 경로와 가능성 발견               | 즉각적인 보상 최적화                |
| **단점**     | 최적 경로를 놓칠 수 있음                 | 새로운 정보 습득 어려움             |

## 4.5 **탐험과 이용의 균형**

강화학습에서 중요한 점은 **탐험(Exploration)**과 **이용(Exploitation)**의 균형입니다. 항상 이용만 하게 되면 최적의 경로를 반복하게 되어 새로운 가능성을 놓칠 수 있고, 반대로 항상 탐험만 하게 되면 효율적으로 목표를 달성하기 어려워집니다.

**입실론-그리디 (ε-Greedy)** 방법은 이 두 가지를 **적절히 조절**하는 방식으로, 에이전트가 **점진적으로 최적의 정책**을 학습할 수 있도록 도와줍니다.

# 5. Markov Decision Process (MDP)

이 문서는 강화학습의 핵심이 되는 **Markov Decision Process (MDP)**의 구조와 개념을 정리한 자료입니다.  
특히 **정책(Policy)**과 **상태 전이 확률(Transition Probability)**이 갖는 **Markov 성질**을 시각적으로 설명합니다.

---

## 1️⃣ MDP 구조

### 📌 상태와 행동의 흐름

- 상태(State): \( $s_0$ , $s_1$, $s_2$ \)
- 행동(Action): \( $a_0$ , $a_1$, $a_2$ \)
- 상태 전이(State Transition)는 행동을 통해 이루어짐

✅ 이 흐름은 에이전트가 **상태에서 행동을 선택하고**, 그 결과로 **다음 상태로 전이**되는 MDP의 기본 구조를 보여줍니다.

---

## 2️⃣ Markov Property의 의미

강화학습은 **Markov 가정**을 기반으로 동작합니다.

> "미래는 오직 현재에만 의존한다.  
> 과거는 더 이상 영향을 주지 않는다."

---

## 3️⃣ Policy의 Markov 성질

### 🎯 수식

\[
P($a_1$ | $s_0$, $a_0$, $s_1$) = P($a_1$ | $s_1$ )
\]

### 🔍 해석

- 정책(Policy)은 과거 상태나 행동이 아닌, **현재 상태 \(s_1\)** 만을 기반으로 **다음 행동 \(a_1\)** 을 선택합니다.
- 과거 정보 \(s_0, a_0\)는 정책 결정에 영향을 미치지 않습니다.

✅ 즉, 정책은 **현재 상태에서의 행동 선택 확률**만 정의합니다.

---

## 4️⃣ Transition Probability의 Markov 성질

### 🎯 수식

\[
$(s_2 | s_0, a_0, s_1, a_1) = P(s_2 | s_1, a_1)$
\]

### 🔍 해석

- 다음 상태 \(s_2\)는 오직 **현재 상태 \(s_1\)**와 **현재 행동 \(a_1\)**에만 의존합니다.
- 과거 상태와 행동은 전이 확률에 영향을 주지 않습니다.

✅ 상태 전이 확률도 **Markov 성질**을 따릅니다.

---

## 5️⃣ 강화학습의 목표 🎯

> 강화학습의 핵심 목표는 **Expected Return (기댓 보상)** 을 최대화하는 정책을 찾는 것입니다.

### 🔢 Return 수식

$$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots$$

- ($R_t$): 시점 $t$에서의 보상
- ($\gamma$): 할인율 (0 ≤ γ ≤ 1), 미래 보상의 현재 가치 반영

---

## ✅ 요약 정리

| 항목                  | 설명                                                                 |
|---------------------|----------------------------------------------------------------------|
| *State*           | 환경의 현재 상황 \($s_t$\)                                             |
| *Action*          | 에이전트가 상태에서 취하는 행동 ($a_t$\)                             |
| *Policy*          | 상태에 따라 행동을 선택하는 전략                       |
| *Transition Probability* | 행동 후 다음 상태로 전이될 확률                 |
| *Markov Property* | 다음 상태는 현재 상태와 행동에만 의존                              |
| *Return \($G_t$)*  | 누적 보상의 합, 미래 보상에 할인율 적용                             |
| **목표**            | Return을 최대화하는 최적 정책 \($\pi^*$\) 학습                         |

---

> 💡 핵심 정리:  
MDP에서는 에이전트가 현재 상태만을 고려하여 행동을 선택하며, 전이 또한 현재 정보만으로 결정됩니다.  
강화학습의 목표는 이 구조를 활용하여 **미래 보상의 합(Return)을 최대화**하는 정책을 찾는 것입니다.  
# 6-1. Ballman Equation 
## about State value function & Action value function

## ✅ 개념 정리

### 🔹 State Value Function (상태 가치 함수)

- **정의**:  
  현재 상태에서 앞으로 기대되는 누적 보상 (Return)의 기대값
- **의미**:  
  지금 상태가 얼마나 "좋은지"를 평가하는 함수  
- **수식**:
  \[
  $V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]$
  \]

- **직관**:  
  큰 값을 가질수록 좋은 상태다 (좋은 정책이면 좋을 상태로 이동해야 함)

---

### 🔹 Action Value Function (행동 가치 함수)

- **정의**:  
  특정 상태에서 특정 행동을 했을 때 앞으로 기대되는 누적 보상의 기대값
- **의미**:  
  이 행동을 지금 이 상태에서 선택했을 때 얼마나 좋은가?
- **수식**:
  \[
 $Q^\pi(s, a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ]$
  \]

---

## 🎯 Return 정의

\[
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$
\]

- \( $\gamma$ \): discount factor (0 < γ ≤ 1)  
- 미래 보상의 현재 가치 반영

---

## 📐 수식적 정의

1. 상태 가치 함수 ( State value function ):

\[
$V(s_t) = \int_{a_t}^\infty G_t(a_t, s_{t+1}, a_{t+1}, \dots \mid s_t) \, da_t$
\]

2. 행동 가치 함수 ( Action value function ):

\[
$Q(s_t, a_t) = \int_{s_{t+1}}^\infty G_t(s_{t+1}, a_{t+1}, s_{t+2}, a_{t+2}, \dots \mid s_t, a_t) \, ds_{t+1}$
\]

> 각 수식은 기대값 관점에서 **미래의 전체 trajectory**를 고려하는 연속 확률 표현입니다.

---

## 🏁 Optimal Policy란?

- 최적 정책은 **상태 가치 함수**를 **최대한으로 만드는 정책**입니다.
- 수식적으로는:

\[
$\pi^* = \arg\max_\pi V^\pi(s)$
\]

또는 아래처럼 **상태-행동 전이 확률 시퀀스를 통해 최대화하는** 정책이 최적 정책이 됩니다:

\[
$\pi^* \propto \max \left( 
P(a_t, s_t),\ 
P(a_{t+1}, s_{t+1}),\ 
\dots,\ 
P(a_\infty, s_\infty)
\right)$
\]

---

## 🧠 요약

| 항목 | 의미 | 수식 | 설명 |
|------|------|------|------|
| \( $V(s)$ \) | 상태 가치 | \( $\mathbb{E}[G_t \mid s] $\) | 상태만 보고 가치 판단 |
| \( $Q(s, a)$ \) | 행동 가치 | \( $\mathbb{E}[G_t \mid s, a] $\) | 상태 + 행동 조합으로 가치 판단 |
| \($\pi^*$\) | 최적 정책 | \( $\arg\max_\pi V^\pi(s) $\) | 가장 좋은 상태를 만드는 정책 |  
  
    

# 6-1 + Specification: Bellman Equation Detailed Analysis

## Return 정의

\[
$G_t \triangleq R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots$
\]

### 1. 상태 가치 함수 \(V(S_t)\)

상태 가치 함수 $V(S_t)$는 현재 상태 $S_t$에서 시작하는 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t P(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

여기서 $P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t)$는 상태 전이 확률이며, $P(a_t \mid S_t)$는 주어진 상태에서의 정책에 의해 선택된 행동의 확률입니다.

## 과정 분석

### 상태 전이 및 반환값 계산

\[
$\int_{a_t, S_{t+1}} G_t P(S_{t+1}, a_{t+1}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 $Q(S_t, a_t)$를 사용하여 다음과 같이 재표현할 수 있습니다:

\[
$\int_{a_t} Q(S_t, a_t) P(a_t \mid S_t) da_t$
\]

### 행동 가치 함수 계산

\[
$\int_{a_t, S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_t, S_{t+1} \mid S_t) da_t, S_{t+1}$
\]

이 과정을 통해 $V(S_t)$에서 $V(S_{t+1})$로의 전이가 이루어지며, 상태 간의 전이 확률 $P(S_{t+1} \mid S_t, a_t)$와 정책 $P(a_t \mid S_t)$을 반영합니다.

## 행동 가치 함수 \( $Q(S_t, a_t)$ \) 계산

행동 가치 함수 $Q(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 기대 반환값을 계산합니다.

\[
$Q(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

### 계산 과정

1. **첫 번째 단계의 계산:**

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(a_{t+1}, \ldots \mid S_{t+1}) dS_{t+1} : a_\infty \Rightarrow V(S_{t+1})$
\]

\[
$\int_{S_{t+1}} (R_t + \gamma V(S_{t+1})) P(S_{t+1} \mid S_t, a_t) dS_{t+1}$
\]

2. **두 번째 단계의 계산:**

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma G_{t+1}) P(S_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) dS_{t+2} : a_\infty \Rightarrow Q(S_{t+1}, a_{t+1})$
\]

\[
$\int_{a_{t+1}, S_{t+1}} (R_t + \gamma Q(S_{t+1}, a_{t+1})) P(S_{t+1}, a_{t+1} \mid S_t, a_t) da_{t+1}, S_{t+1}$
\]

이러한 수식적 분석을 통해 벨만 방정식의 재귀적 특성과 가치 함수 간의 관계를 정의할 수 있습니다.

# Optimal Policy Analysis

## 상태 가치 함수 \( $V(S_t)$ \)

상태 가치 함수 $V(S_t)$는 현재 상태에서 시작하여 취할 수 있는 모든 행동의 기대 반환값을 계산합니다.

\[
$V(S_t) \triangleq \int_{a_t} G_t(a_t, S_{t+1}, a_{t+1}, \ldots \mid S_t) da_t : a_\infty$
\]

이는 최적의 행동 가치 함수 $Q^*(S_t, a_t)$를 사용하여 다음과 같이 표현됩니다:

\[
$V(S_t) = \int_{a_t} Q^*(S_t, a_t) P(a_t \mid S_t) da_t$
\]

여기서 $argmax P(a_t \mid S_t)$는 주어진 상태에서 최적의 행동을 선택합니다.

## 최적 행동 가치 함수 \( $Q^*(S_t, a_t)$ \)

최적 행동 가치 함수 $Q^*(S_t, a_t)$는 주어진 상태와 행동에서 시작하는 최대 기대 반환값을 계산합니다.

\[
$Q^*(S_t, a_t) \triangleq \int_{S_{t+1}}^\infty G_t P(S_{t+1}, a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_t, a_t) dS_{t+1} : a_\infty$
\]

이는 모든 가능한 이후 상태의 시퀀스를 고려하여 계산됩니다:

\[
$Q^*(S_t, a_t) = P(a_{t+1}, S_{t+2}, a_{t+2}, \ldots \mid S_{t}, a_t) P(S_{t+1} \mid S_t, a_t)$
\]

\[
= $P(S_{t+2}, a_{t+2}, \ldots \mid S_{t+1}, a_{t+1}) P^*(a_{t+1} \mid S_{t+1})$
\]

\[
$...$
\]

## 7. Q - value 는 어떻게 찾을까? ( 강화학습에서의 Q 값 추정 )

# Monte-Carlo (MC) vs Temporal Difference (TD)

강화학습에서 $Q$-value를 추정하는 대표적인 두 가지 방법인 **Monte-Carlo(MC)** 방식과 **Temporal Difference(TD)** 방식은 서로 다른 철학을 갖고 접근

---

## 🔹 Monte-Carlo 방법 (MC)

- **정의**: 전체 에피소드가 끝난 후, 실제 관측된 누적 보상 $G_t$를 평균하여 $Q$ 값을 추정
- **수식**:
  $Q(s_t, a_t) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$
- **특징**:
  - 전체 trajectory(궤적)를 따라가며 실제 리턴 $G_t$를 계산
  - 모델 없이 순수한 경험 기반 학습
- **장점**:
  - 불편추정(unbiased estimate)
  - 간단하고 직관적
- **단점**:
  - 에피소드가 끝날 때까지 기다려야 함 (느림)
  - 온라인 학습에 부적합

---

## 🔹 Temporal Difference 방법 (TD)
모델이 없거나(model-free), 모델 역학(model dynamics)을 미리 알 필요가 없는 비 에피소드 작업에도 적용 가능한 학습 알고리즘
- **정의**: 다음 상태의 $Q$ 값을 기반으로 한 단계씩 근사 추정
- **수식**:  

  $Q(s_t, a_t) \approx R_t + \gamma Q(s_{t+1}, a_{t+1})$
  또는 평균을 사용한 업데이트 방식:  

  $Q_N = \frac{1}{N} \sum_{i=1}^{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) \right)$
  그리고 점진적 업데이트:  
  
  $Q_N = Q_{N-1} + \frac{1}{N} \left( R_t^{(i)} + \gamma Q(s_{t+1}^{(i)}, a_{t+1}^{(i)}) - Q_{N-1} \right)$
- **특징**:
  - 리턴 전체를 기다리지 않고, 예측된 다음 $Q$ 값을 사용하여 즉시 업데이트
  - 학습 속도가 빠르고 실시간 학습에 적합
- **장점**:
  - 에피소드 종료를 기다릴 필요 없음 (빠름)
  - 온라인 학습 가능
- **단점**:
  - 추정값을 바탕으로 업데이트되므로 bias가 존재할 수 있음
  -  Monte Carlo 방법은 에피소드 환경에만 적용된다는 단점과 에피소드가 매우 길면 가치 함수를 계산하기 위해 오랜 시간이 걸림

---

## 📊 비교 정리

| 항목 | Monte-Carlo (MC) | Temporal Difference (TD) |
|------|------------------|---------------------------|
| 학습 시점 | 에피소드 종료 후 | 한 스텝마다 (온라인) |
| 보상 사용 | 실제 누적 보상 $G_t$ | 예상된 $Q(s_{t+1}, a_{t+1})$ |
| 추정 성질 | 불편추정 (Unbiased) | 편향추정 (Biased) |
| 데이터 기반 | 완전한 경험 | 예측 포함 |
| 장점 | 안정적, 직관적 | 빠름, 실시간 학습 가능 |
| 단점 | 느림, 오프라인 | 초기 불안정, 편향 가능성 |

---

## ✅ 결론

- Monte-Carlo는 **정확하지만 느리며**, 에피소드 단위로 학습합니다.
- Temporal Difference는 **빠르지만 추정 기반**으로 더 빠른 반응과 실시간 처리가 가능합니다.
- 실제 강화학습에서는 이 두 방식을 적절히 혼합한 **SARSA, Q-learning** 같은 알고리즘들이 널리 사용됩니다.

# 8. 강화학습 방법


강화학습은 에이전트가 환경과 상호작용하며 최적의 정책(policy)을 학습하는 방법이다. 이 학습 과정은 **정책 실행을 통한 샘플 생성**, **가치함수 추정**, **정책 개선**의 **3단계**를 반복하는 **iteration 구조**로 이루어진다.

---

## ✅ 강화학습의 3단계 반복 구조

### 1. 정책 실행을 통한 샘플 생성
에이전트가 현재의 정책을 바탕으로 환경에서 행동을 수행하며, 상태(state), 행동(action), 보상(reward), 다음 상태(next state) 등의 **경험 데이터(experience)** 또는 **전이(transition)** 를 수집한다. 이러한 (s, a, r, s') 튜플을 **샘플**이라고 부른다.

### 2. 가치 함수 추정
수집된 경험 샘플을 바탕으로 **가치 함수(value function)** 또는 **행동 가치 함수(action-value function, Q-function)** 를 추정한다. 이는 각 상태나 상태-행동 쌍에 대한 기대 보상을 평가하는 함수이다.

### 3. 정책 개선
추정된 가치 함수에 기반하여 정책을 수정하거나, 가치가 높은 행동을 선택하도록 정책을 조정한다. 이로써 더 나은 행동 선택이 가능해진다.

이 3단계를 **반복(iteration)** 하여 정책을 점점 최적화해간다.

---

## 🧮 강화학습의 수학적 목적

강화학습의 목적은 다음과 같이 수학적으로 정의된다:

\[
$\theta^* = \arg\max_{\theta} J(\theta)$
\]

\[
$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right]$
\]

- \( $J(\theta)$ \): 정책에 대한 기대 보상
- \( $r(x_t, u_t)$ \): 시간 t에서 상태 \(x_t\)에서 행동 \(u_t\)를 했을 때 받는 보상
- \( $\gamma$ \): 할인율 (discount factor)
- \( $p_\theta(\tau)$ \): 정책에 의해 생성된 trajectory \( \tau \)의 확률분포

이러한 수식을 통해 강화학습의 목표는 **기댓값이 최대가 되는 정책 파라미터 \($\theta$\)** 를 찾는 것이다.

---

## 🔁 Trajectory : 정책에 따라 에이전트가 환경과 상호작용하며 만들어낸 상태와 행동의 연속된 순서 표현

\[
$\tau = (x_0, u_0, x_1, u_1, ..., x_T, u_T)$
\]

\( $\tau$ \)는 정책 \( $\pi_\theta$ \) 에 의해 생성되는 상태-행동의 시퀀스이며, 일반적으로 신경망(neural network)을 통해 정책을 표현한다.

---

## 🤖 MDP 프레임워크에서의 상호작용 구조

Agent와 환경(Environment) 간 상호작용은 MDP(Markov Decision Process) 프레임워크로 표현할 수 있다.

- 에이전트는 상태 \(x_t\)를 관찰하고, 정책 \(\pi_\theta(u_t | x_t)\)에 따라 행동 \(u_t\)를 선택한다.
- 환경은 행동 \(u_t\)에 따라 보상 \(r_t\)와 다음 상태 \(x_{t+1}\)를 반환한다.
- 이러한 상호작용은 에피소드가 끝날 때까지 반복된다.

### 전체 반환 (Return):

\[
$G_0 = \sum_{t=0}^{T} \gamma^t r(x_t, u_t)$
\]

이는 시간 t=0부터 에피소드 종료 시점까지 받을 수 있는 **discounted reward의 총합**이다.

### 특정 시점 k부터의 반환:

\[
$G_t = \sum_{k=t}^{T} \gamma^{k-t} r(x_k, u_k)$
\]

이는 시간 t부터 받을 수 있는 누적 보상(return)을 의미한다.

---

## 📌 정리

- 강화학습은 반복적으로 정책을 개선하는 최적화 문제이다.
- 기대 보상을 최대화하는 방향으로 정책 파라미터를 조정한다.
- 대표적인 두 방법은 다음과 같다:
  - **Policy Gradient**: 정책을 직접 매개변수화하여 최적화 (예: Actor-Critic)
  - **Value-based**: 가치함수를 추정하여 간접적으로 정책 유도 (예: DQN)

## 9. 강화학습의 목적함수 및 정책 그래디언트 정리

강화학습에서 에이전트의 목표는 주어진 정책 $\pi_\theta$ 하에서 얻을 수 있는 기대 보상, 즉 목적함수 $J(\theta)$를 최대화하는 것이다.

---

#### 1. Trajectory의 확률 밀도 함수

Chain rule을 이용하여 trajectory $\tau$에 대한 확률 밀도 함수 $p_\theta(\tau)$는 아래와 같이 전개된다.

$p_\theta(\tau) = p(x_0, u_0, x_1, u_1, \dots, x_T, u_T)
= p(x_0)p_\theta(u_0, x_1, u_1, \dots, x_T, u_T \mid x_0)$

초기 상태 변수 $x_0$의 확률 밀도 함수는 정책 $\pi_\theta$와 무관하므로 $p(x_0)$로 표현한다. 위 식의 두 번째 줄에도 chain rule을 연달아 적용하면 다음과 같다.

$$\begin{align*}
p_\theta(\tau) &= p(x_0)p_\theta(u_0 \mid x_0)p(x_1 \mid x_0, u_0)p_\theta(u_1 \mid x_0, u_0, x_1) \\
&\quad \cdot p(x_2 \mid x_0, u_0, x_1, u_1)p_\theta(u_2 \mid x_0, \dots)p(x_3 \mid \dots) \dots
\end{align*}$$  


Markov 시퀀스 가정에 의해 아래와 같이 간단화할 수 있다:

$p_\theta(u_1 \mid x_0, u_0, x_1) = \pi_\theta(u_1 \mid x_1) \\
p(x_2 \mid x_0, u_0, x_1, u_1) = p(x_2 \mid x_1, u_1)$

따라서 최종적으로 아래와 같은 형태가 된다:

$p_\theta(\tau) = p(x_0) \prod_{t=0}^{T} \pi_\theta(u_t \mid x_t)p(x_{t+1} \mid x_t, u_t)$

---

#### 2. 목적 함수 $J(\theta)$ 정의

목적 함수는 state value function과도 관계가 있으며, 다음과 같이 정의된다:

$J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right]$

적분으로 표현하면:

$J(\theta) = \int_\tau p_\theta(\tau) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau$

Trajectory $\tau$를 다음과 같이 분할할 수 있다:

$\tau = (x_0, u_0, x_1, u_1, \dots, x_T, u_T) = (x_0) \cup \tau_{u_0:u_T}$

Chain rule에 의해:

$p_\theta(\tau) = p_\theta(x_0, \tau_{u_0:u_T}) = p_\theta(x_0) p_\theta(\tau_{u_0:u_T} \mid x_0)$  

이를 위 목적 함수에 대입하면:

$$\begin{align*}
J(\theta) &= \int_{x_0} \int_{\tau_{u_0:u_T}} p_\theta(x_0) p_\theta(\tau_{u_0:u_T} \mid x_0) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau_{u_0:u_T} dx_0 \\
&= \int_{x_0} \left[ \int_{\tau_{u_0:u_T}} p_\theta(\tau_{u_0:u_T} \mid x_0) \left( \sum_{t=0}^{T} \gamma^t r(x_t, u_t) \right) d\tau_{u_0:u_T} \right] p(x_0) dx_0
\end{align*}$$  


대괄호 항은 상태가 $x_0$일 때의 **state value function**이므로 목적 함수는:

$J(\theta) = \int_{x_0} V^{\pi_\theta}(x_0) p(x_0) dx_0 = \mathbb{E}_{x_0 \sim p(x_0)}[V^{\pi_\theta}(x_0)]$  


---

#### 즉, 목적 함수 $J(\theta)$는 초기 상태 변수 $x_0$에 대한 상태 가치의 평균값이며, 강화학습에서 에이전트가 최대화하고자 하는 대상이다.

---

#### 3. 목적 함수의 그래디언트 계산

$\nabla_\theta J(\theta) = \nabla_\theta \int_\tau p_\theta(\tau) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right) d\tau$  


log-미분 성질 사용:


= $\int_\tau p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right) d\tau$

Trajectory 확률의 log-그래디언트는 다음과 같다:

$\nabla_\theta \log p_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t)$  


따라서 정책 그래디언트 정리는 다음과 같다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\left( \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \right) \left( \sum_{t=0}^T \gamma^t r(x_t, u_t) \right)\right]$  

![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%5Cleft%28%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cright%29%20%5Cleft%28%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cgamma%5Et%20r%28x_t%2C%20u_t%29%20%5Cright%29%5Cright%5D)  


---

#### 4. 인과성 반영 (Causality 고려)

시간 $t$에 선택된 행동은 그 이후 보상에만 영향을 준다는 인과성 원칙을 반영하면 다음과 같이 수정된다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \left( \sum_{k=t}^T \gamma^{k - t} r(x_k, u_k) \right)\right]$  


![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cleft%28%20%5Csum_%7Bk%3Dt%7D%5ET%20%5Cgamma%5E%7Bk%20-%20t%7D%20r%28x_k%2C%20u_k%29%20%5Cright%29%5Cright%5D)  



---

#### 5. 감가율 $\gamma$의 영향

감가율 $\gamma^t$는 log-policy gradient에 곱해지기 때문에 시간이 지남에 따라 기여도가 점점 작아진다.

- $\gamma \to 0$: 미래 보상 영향 감소 → 빠르게 수렴하지만 후반 정보 손실
- $\gamma = 1$: 전 보상에 평등한 중요도 → variance 커질 수 있음

---

#### 6. 실용적인 정책 그래디언트 형태

보다 안정적인 학습을 위해, 실용적인 정책 그래디언트는 다음과 같은 형태로 사용된다:

$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(u_t \mid x_t) \left( \sum_{k=t}^T \gamma^{k-t} r(x_k, u_k) \right) \right]$  

![equation](https://latex.codecogs.com/svg.latex?\color{white}%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%28%5Ctau%29%7D%5Cleft%5B%20%5Csum_%7Bt%3D0%7D%5ET%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%28u_t%20%5Cmid%20x_t%29%20%5Cleft%28%20%5Csum_%7Bk%3Dt%7D%5ET%20%5Cgamma%5E%7Bk-t%7D%20r%28x_k%2C%20u_k%29%20%5Cright%29%20%5Cright%5D)  



위의 형태는 **REINFORCE 알고리즘**의 핵심이며, 모델-프리 강화학습에서 널리 사용된다.

# 10. 함수 근사와 딥 강화학습

## 함수 근사(Function Approximation)의 필요성

실제 문제에서는 상태 공간이 너무 커서 표 형태로 가치 함수나 정책을 표현하기 어렵습니다. 이때 함수 근사를 사용합니다:

- **정의**: 복잡한 함수(가치 함수, 정책)를 단순화된 매개변수 집합으로 표현
- **방법**: 선형 함수, 신경망, 의사결정 트리 등
- **장점**: 
  - 일반화 능력 (보지 못한 상태에 대해서도 예측 가능)
  - 연속적인 상태 및 행동 공간 처리 가능
  - 메모리 효율성

## 딥 강화학습(Deep Reinforcement Learning)

신경망을 함수 근사기로, 특히 딥러닝 기술을 활용하는 강화학습:

- **DQN (Deep Q-Network)**: Q 함수를 딥러닝으로 근사
- **DDPG (Deep Deterministic Policy Gradient)**: 연속 행동 공간을 위한 액터-크리틱 구조
- **PPO (Proximal Policy Optimization)**: 정책 최적화를 위한 신뢰 영역 방법
- **SAC (Soft Actor-Critic)**: 엔트로피 최대화를 통한 탐험 강화

## 경험 리플레이(Experience Replay)

강화학습의 샘플 효율성과 학습 안정성을 높이기 위한 기법:

- **정의**: 에이전트의 경험(s, a, r, s')을 메모리 버퍼에 저장하고 학습에 재사용
- **과정**:
  1. 경험 수집 및 버퍼에 저장
  2. 무작위로 미니배치 샘플링
  3. 샘플을 사용하여 가치 함수 또는 정책 업데이트
- **장점**:
  - 데이터 효율성 향상
  - 시간적 상관관계 감소
  - 학습 안정성 증가

## 멀티 에이전트 강화학습(Multi-Agent RL)

여러 에이전트가 동시에 상호작용하는 환경에서의 강화학습:

- **환경 유형**:
  - 협력적: 에이전트들이 공통 목표를 위해 협력
  - 경쟁적: 에이전트들이
  - 혼합형: 협력과 경쟁이 공존
- **도전 과제**:
  - 비정상성(non-stationarity): 다른 에이전트들의 행동 변화로 환경이 동적으로 변함
  - 신용 할당 문제: 팀 성공에 대한 개별 에이전트의 기여도 평가
- **주요 알고리즘**:
  - MADDPG (Multi-Agent DDPG)
  - QMIX: 협력적 Q-학습
  - MAPPO (Multi-Agent PPO)

## 계층적 강화학습(Hierarchical RL)

복잡한 문제를 계층적으로 분해하여 해결하는 방법:

- **구조**:
  - 상위 정책: 추상적인 목표나 서브태스크 선택
  - 하위 정책: 상위 목표를 달성하기 위한 구체적 행동 실행
- **장점**:
  - 장기적 의존성 처리 용이
  - 탐험 효율성 향상
  - 전이 학습(transfer learning) 가능
- **알고리즘**:
  - Option Framework
  - Feudal Networks
  - HIRO (Hierarchical Reinforcement Learning with Off-policy correction)

## 메타 강화학습(Meta RL)

빠르게 적응하는 능력을 배우는 "학습하는 방법을 학습":

- **목표**: 다양한 작업에서 경험을 활용하여 새로운 환경에 빠르게 적응
- **방법**:
  - 외부 루프: 태스크 분포에 대한 학습
  - 내부 루프: 특정 태스크에 적응
- **알고리즘**:
  - Model-Agnostic Meta-Learning (MAML)
  - RL²: Recurrent policy로 메타 학습
  - PEARL: 확률적 임베딩을 통한 적응적 강화학습

## 강화학습의 실제 응용

- **로봇 제어**: 운동 기술, 조작, 내비게이션
- **게임 AI**: AlphaGo, OpenAI Five, AlphaStar
- **자율주행**: 경로 계획, 제어 정책
- **자원 관리**: 데이터 센터 에너지 최적화, 스마트 그리드
- **의료**: 개인화된 치료, 임상 의사결정 지원
- **금융**: 포트폴리오 관리, 알고리즘 거래

## 강화학습의 한계와 도전 과제

- **샘플 효율성**: 실제 환경에서 많은 시도가 필요
- **탐험-이용 트레이드오프**: 최적의 균형 찾기 어려움
- **신용 할당 문제**: 지연된 보상의 원인 파악
- **일반화**: 학습 환경과 다른 환경에서의 성능
- **안전성**: 위험한 행동 방지, 안전 제약 준수
- **해석 가능성**: 학습된 정책의 이해와 설명
- **현실 갭**: 시뮬레이션과 실제 환경 간의 차이
